{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: Sunday, December 17 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "* Reviews: Dies Natalis Solis Invicti ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* _Sutton & Barto, Reinforcement Learning: An Introduction_ [2nd Edition]\n",
    "* [_Reinforcement Learning - Developing Intelligent Agents_](https://www.youtube.com/playlist?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from abc import ABC, abstractmethod\n",
    "from copy import deepcopy\n",
    "from itertools import combinations\n",
    "from random import randint, random\n",
    "from tqdm import trange\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tic-Tac-Toe game definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    \"\"\"\n",
    "    Class representing the Tic-Tac-Toe game.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, board: np.ndarray = None) -> None:\n",
    "        \"\"\"\n",
    "        Constructor of the Tic-Tac-Toe game.\n",
    "\n",
    "        Args:\n",
    "            board: the board of the game in a given state.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        # if board is None\n",
    "        if board is None:\n",
    "            # create the starting board of the game\n",
    "            board = np.ones((3, 3), dtype=np.uint8) * -1\n",
    "        # define the board game\n",
    "        self._board = board\n",
    "        # define a board to ease the check winner computation\n",
    "        self._eqv_board = np.array([[1, 6, 5], [8, 4, 0], [3, 2, 7]], dtype=np.uint8)\n",
    "        # define a mapping for pretty printing\n",
    "        self._id_to_block = {-1: 'â¬œï¸', 0: 'âŒ', 1: 'â­•ï¸'}\n",
    "\n",
    "    @property\n",
    "    def board(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns a copy of the game board.\n",
    "\n",
    "        Args:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            A copy of the game board is returned.\n",
    "        \"\"\"\n",
    "        # return a copy of the board so that the board cannot be modified from outside\n",
    "        return deepcopy(self._board)\n",
    "\n",
    "    def print(self) -> None:\n",
    "        \"\"\"\n",
    "        Print the table in a pretty way.\n",
    "\n",
    "        Args:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        # define a board for pretty printing\n",
    "        fancy_board = np.chararray(self._board.shape, itemsize=1, unicode=True)\n",
    "        for i in range(fancy_board.shape[0]):\n",
    "            for j in range(fancy_board.shape[1]):\n",
    "                # fill the fancy board\n",
    "                fancy_board[(i, j)] = self._id_to_block[self._board[(i, j)]]\n",
    "        print(fancy_board)\n",
    "\n",
    "    def check_winner(self) -> Literal[-1, 0, 1]:\n",
    "        \"\"\"\n",
    "        Check the winner.\n",
    "\n",
    "        Args:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            0 is returned if the first player has won;\n",
    "            1 is returned if the second player has won;\n",
    "            -1 is returned if no one has won.\n",
    "        \"\"\"\n",
    "        # take the tiles belonging to the first player\n",
    "        player1_tiles = self._board == 0\n",
    "        # take the tiles belonging to the second player\n",
    "        player2_tiles = self._board == 1\n",
    "        # check if the first player has won\n",
    "        if any(sum(h) == 12 for h in combinations(self._eqv_board[player1_tiles], 3)):\n",
    "            return 0\n",
    "        # check if the second player has won\n",
    "        if any(sum(h) == 12 for h in combinations(self._eqv_board[player2_tiles], 3)):\n",
    "            return 1\n",
    "        # no player has won\n",
    "        return -1\n",
    "\n",
    "    def is_still_playable(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the board game contains not taken tiles.\n",
    "\n",
    "        Args:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            A boolean representing if the game is still playable\n",
    "            is returned.\n",
    "        \"\"\"\n",
    "        # check if still there are not taken tiles\n",
    "        return any((self._board == -1).flatten())\n",
    "\n",
    "    def move(self, move: tuple[int, int], player_id: int) -> bool:\n",
    "        \"\"\"\n",
    "        Perform a move for a given player if it is acceptable.\n",
    "\n",
    "        Args:\n",
    "            move: the move to play;\n",
    "            player_id: the id of the moving player.\n",
    "\n",
    "        Returns:\n",
    "            The acceptability of the move is returned.\n",
    "        \"\"\"\n",
    "        # if the player id is not valid\n",
    "        if player_id >= 2 or player_id <= -1:\n",
    "            return False\n",
    "        # check if the move is acceptable\n",
    "        acceptable = self.is_acceptable(move)\n",
    "        # if it is\n",
    "        if acceptable:\n",
    "            # update the board\n",
    "            self._board[move] = player_id\n",
    "        return acceptable\n",
    "\n",
    "    def is_acceptable(self, move: tuple[int, int]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a move is acceptable.\n",
    "\n",
    "        Args:\n",
    "            move: the move to play.\n",
    "\n",
    "        Returns:\n",
    "            The acceptability of the move is returned.\n",
    "        \"\"\"\n",
    "        acceptable: bool = move[0] >= 0 and move[0] <= 3 and move[1] >= 0 and move[1] <= 3 and self._board[move] < 0\n",
    "        return acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tic-Tac-Toe match definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(game: 'TicTacToe', player1: 'Player', player2: 'Player', log: bool = False) -> Literal[-1, 0, 1]:\n",
    "    \"\"\"\n",
    "    Play a game between two given players.\n",
    "\n",
    "    Args:\n",
    "        game: a Tic-Tac-Toe game instance;\n",
    "        player1: the player who starts the game;\n",
    "        player2: the second player of the game;\n",
    "        log: a boolean flag to print the match log or not.\n",
    "\n",
    "    Returns:\n",
    "        0 is returned if the first player has won;\n",
    "        1 is returned if the second player has won;\n",
    "        -1 is returned if no one has won.\n",
    "    \"\"\"\n",
    "    # if the user wants to see the full game\n",
    "    if log:\n",
    "        game.print()\n",
    "    # define the players\n",
    "    players = [player1, player2]\n",
    "    # set the moving player index\n",
    "    current_player_idx = 1\n",
    "    # define a variable to indicate if there is a winner\n",
    "    winner = -1\n",
    "    # if we can still play\n",
    "    while winner < 0 and game.is_still_playable():\n",
    "        # update the current moving player index\n",
    "        current_player_idx += 1\n",
    "        current_player_idx %= len(players)\n",
    "        # define a variable to check if the chosen move is ok or not\n",
    "        ok = False\n",
    "        # while the chosen move is not ok\n",
    "        while not ok:\n",
    "            # let the current player make a move\n",
    "            move = players[current_player_idx].make_move(game)\n",
    "            # check if now it is ok\n",
    "            ok = game.move(move, current_player_idx)\n",
    "        # if the user wants to see the full game\n",
    "        if log:\n",
    "            game.print()\n",
    "        # check if there is a winner\n",
    "        winner = game.check_winner()\n",
    "    # if the user wants to see the full game\n",
    "    if log:\n",
    "        if winner == -1:\n",
    "            print(f\"Draw!\")\n",
    "        else:\n",
    "            print(f\"Winner: Player {winner}\")\n",
    "    # return the winner\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions\n",
    "\n",
    "The `show_statistics` function is used to print some statistics about a batch of matches played between two players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_statistics(player_id: int, player1: 'Player', player2: 'Player', n_matches: int = 1_000) -> None:\n",
    "    \"\"\"\n",
    "    Play a certain number of games between two players and print the statistics.\n",
    "\n",
    "    Args:\n",
    "        player_id: the player whose statistics we want to track;\n",
    "        player1: the player who starts the game;\n",
    "        player2: the second player of the game;\n",
    "        n_matches: the number of matches to play.\n",
    "\n",
    "    Return:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    counter_wins = 0\n",
    "    counter_losses = 0\n",
    "    counter_draws = 0\n",
    "    for _ in range(n_matches):\n",
    "        game = TicTacToe()\n",
    "        winner = play(game, player1, player2)\n",
    "        counter_wins = counter_wins + 1 if winner == player_id else counter_wins\n",
    "        counter_losses = counter_losses + 1 if winner == (player_id + 1) % 2 else counter_losses\n",
    "        counter_draws = counter_draws + 1 if winner == -1 else counter_draws\n",
    "    print(f'Over {n_matches} matches: {counter_wins} wins, {counter_losses} losses and {counter_draws} draw')\n",
    "    print(f'Wins + Draws percentage: {(counter_wins + counter_draws) / n_matches:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract Player definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(ABC):\n",
    "    \"\"\"\n",
    "    Class representing an abstract player.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Player constructor to be implemented.\n",
    "\n",
    "        Args:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_move(self, game: 'TicTacToe') -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Abstract Method for deciding which move to play.\n",
    "\n",
    "        Args:\n",
    "            game: a Tic-Tac-Toe game instance.\n",
    "\n",
    "        Returns:\n",
    "            A move is returned.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Player definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer(Player):\n",
    "    \"\"\"\n",
    "    Class representing a player who plays randomly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Constructor of the random player.\n",
    "\n",
    "        Args:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def make_move(self, game: TicTacToe) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Pick a randome move.\n",
    "\n",
    "        Args:\n",
    "            game: a Tic-Tac-Toe game instance.\n",
    "\n",
    "        Returns:\n",
    "            A random move is returned.\n",
    "        \"\"\"\n",
    "        return (randint(0, game.board.shape[0] - 1), randint(0, game.board.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-learning** is a reinforcement learning technique which is based on updating the action-value function $Q(s_t, a_t)$ according to the following formula:\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow (1 - \\alpha) * Q(s_t, a_t) + \\alpha * ( R_{t+1} + \\gamma * \\max_a Q(s_{t+1}, a) )\n",
    "$$\n",
    "where $\\gamma$ is the _discount rate_ and $\\alpha$ is a learning rate parameter. \\\n",
    "Here $s_{t+1}$ is supposed to be a state in which our player is again called upon to make a move. \\\n",
    "Since in our case $s_{t+1}$ is a state in which the opponent has to play, we should take this into account by putting a _minus_ sign in front of $\\max_a Q(s_{t+1}, a) )$. Thus, the update formula becomes:\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow (1 - \\alpha) * Q(s_t, a_t) + \\alpha * ( R_{t+1} + \\gamma * ( - \\max_a Q(s_{t+1}, a) ) )\n",
    "$$\n",
    "> Note: This idea came to [Davide Vitabile](https://github.com/Vitabile/Computational-Intelligence/tree/main)'s mind\n",
    "\n",
    "The cells below define a Q-learning player and train it against a random player. \\\n",
    "The obtained results are also shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningRLPlayer(Player):\n",
    "    \"\"\"\n",
    "    Class representing player who learns to play thanks to the Q-learning technique.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_episodes: int,\n",
    "        alpha: float,\n",
    "        gamma: float,\n",
    "        min_exploration_rate: float,\n",
    "        exploration_decay_rate: float,\n",
    "        opponent: 'Player',\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        The Q-learning player constructor.\n",
    "\n",
    "        Args:\n",
    "            n_episodes: the number of episodes for the training phase;\n",
    "            alpha: how much information to incorporate from the new experience;\n",
    "            gamma: the discount rate of the Bellman equation;\n",
    "            min_exploration_rate: the minimum rate for exploration during the training phase;\n",
    "            exploration_decay_rate: the exploration decay rate used during the training;\n",
    "            opponent: the opponent to play against.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._q_table = {}  # define the Action-value function\n",
    "        self._n_episodes = n_episodes  # define the number of episodes for the training phase\n",
    "        self._alpha = alpha  # define how much information to incorporate from the new experience\n",
    "        self._gamma = gamma  # define the discount rate of the Bellman equation\n",
    "        self._exploration_rate = 1  # define the exploration rate for the training phase\n",
    "        self._min_exploration_rate = (\n",
    "            min_exploration_rate  # define the minimum rate for exploration during the training phase\n",
    "        )\n",
    "        self._exploration_decay_rate = (\n",
    "            exploration_decay_rate  # define the exploration decay rate used during the training\n",
    "        )\n",
    "        self._opponent = opponent  # define the opponent to play against\n",
    "\n",
    "    def _move_reward(self, game: 'TicTacToe', move: tuple[int, int], player_id: int) -> tuple[Literal[-1, 1], bool]:\n",
    "        \"\"\"\n",
    "        Try a move and return the corresponding reward.\n",
    "\n",
    "        Args:\n",
    "            game: a Tic-Tac-Toe game instance;\n",
    "            move: the move to try;\n",
    "            player_id: my player's id.\n",
    "\n",
    "        Returns:\n",
    "            The reward and the acceptability of the move are returned.\n",
    "        \"\"\"\n",
    "        # play a move\n",
    "        acceptable = game.move(move, player_id)\n",
    "        # give a negative reward to the agent\n",
    "        reward = -1\n",
    "        # if the move is acceptable\n",
    "        if acceptable:\n",
    "            # give a positive reward to the agent\n",
    "            reward = 1\n",
    "        return reward, acceptable\n",
    "\n",
    "    def _game_reward(self, player: 'TicTacToe', winner: int) -> Literal[-10, 0, 10]:\n",
    "        \"\"\"\n",
    "        Calculate the reward based on how the game ended.\n",
    "\n",
    "        Args:\n",
    "            player: the winning player;\n",
    "            winner: the winner's player id.\n",
    "\n",
    "        Returns:\n",
    "            The game reward is returned.\n",
    "        \"\"\"\n",
    "        # if there was no winner\n",
    "        if winner == -1:\n",
    "            # return no reward\n",
    "            return 0\n",
    "        # if the agent is the winner\n",
    "        elif self == player:\n",
    "            # give a big positive reward\n",
    "            return 10\n",
    "        # give a big negative reward, otherwise\n",
    "        return -10\n",
    "\n",
    "    def _map_state_to_index(self, game: 'TicTacToe') -> str:\n",
    "        \"\"\"\n",
    "        Given a game state, this function translates it into an index to access the Q_table.\n",
    "\n",
    "        Args:\n",
    "            game: a Tic-Tac-Toe game instance.\n",
    "        \"\"\"\n",
    "        # take the current game state\n",
    "        state = game.board\n",
    "        # change not taken tiles values to 2\n",
    "        state[state == -1] = 2\n",
    "        # map the state to a string in base 3\n",
    "        state_repr_index = ''.join(str(_) for _ in state.flatten())\n",
    "        return state_repr_index\n",
    "\n",
    "    def _update_q_table(self, state_repr_index: str, new_state_repr_index: str, action: int, reward: int) -> None:\n",
    "        \"\"\"\n",
    "        Update the Q_table according to the Q-learning update formula.\n",
    "\n",
    "        Args:\n",
    "            state_repr_index: the current state index;\n",
    "            new_state_repr_index: the next state index;\n",
    "            action: the performed action;\n",
    "            reward: the reward obtained by applying the action on the current state.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        # if the current state is unknown\n",
    "        if state_repr_index not in self._q_table:\n",
    "            # create its entry in the action-value mapping table\n",
    "            self._q_table[state_repr_index] = np.zeros((9,))\n",
    "        # if the next state is unknown\n",
    "        if new_state_repr_index not in self._q_table:\n",
    "            # create its entry in the action-value mapping table\n",
    "            self._q_table[new_state_repr_index] = np.zeros((9,))\n",
    "        prev_value = self._q_table[state_repr_index][action]\n",
    "        # update the action-value mapping entry for the current state using Q-learning\n",
    "        self._q_table[state_repr_index][action] = (1 - self._alpha) * prev_value + self._alpha * (\n",
    "            reward + self._gamma * (-np.max(self._q_table[new_state_repr_index]))\n",
    "        )\n",
    "\n",
    "    def _make_move(self, game: 'TicTacToe') -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Construct a move during the training phase to update the Q_table.\n",
    "\n",
    "        Args:\n",
    "            game: a Tic-Tac-Toe game instance.\n",
    "\n",
    "        Returns:\n",
    "            A move to play is returned.\n",
    "        \"\"\"\n",
    "        # get the current state representation\n",
    "        state_repr_index = self._map_state_to_index(game)\n",
    "\n",
    "        # randomly perform exploration\n",
    "        if random() < self._exploration_rate:\n",
    "            # by returning a random move\n",
    "            move = randint(0, 8)\n",
    "        # perform eploitation, otherwise\n",
    "        else:\n",
    "            # if the current state is unknown\n",
    "            if state_repr_index not in self._q_table:\n",
    "                # create its entry in the action-value mapping table\n",
    "                self._q_table[state_repr_index] = np.zeros((9,))\n",
    "            # take the action with maximum return of rewards\n",
    "            move = np.argmax(self._q_table[state_repr_index])\n",
    "\n",
    "        # reshape the move to match the board shape\n",
    "        move = move // 3, move % 3\n",
    "\n",
    "        return move\n",
    "\n",
    "    def make_move(self, game: 'TicTacToe') -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Construct a move to be played according to the Q_table.\n",
    "\n",
    "        Args:\n",
    "            game: a Tic-Tac-Toe game instance.\n",
    "\n",
    "        Returns:\n",
    "            A move to play is returned.\n",
    "        \"\"\"\n",
    "        # get the current state representation\n",
    "        state_repr_index = self._map_state_to_index(game)\n",
    "        # if the current state is known\n",
    "        if state_repr_index in self._q_table:\n",
    "            # take the action with maximum return of rewards\n",
    "            move = np.argmax(self._q_table[state_repr_index])\n",
    "            # reshape the move to match the board shape\n",
    "            move = move // 3, move % 3\n",
    "            # if the move is acceptable\n",
    "            if game.is_acceptable(move):\n",
    "                # return it\n",
    "                return move\n",
    "        # perform a random move, otherwise\n",
    "        return (randint(0, game.board.shape[0] - 1), randint(0, game.board.shape[1] - 1))\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Train the Q-learning player.\n",
    "\n",
    "        Args:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        # define the history of rewards\n",
    "        all_rewards = []\n",
    "        # define how many episodes to run\n",
    "        pbar = trange(self._n_episodes)\n",
    "        # define the players\n",
    "        players = (self, self._opponent)\n",
    "        # for each episode\n",
    "        for episode in pbar:\n",
    "            # define a new game\n",
    "            game = TicTacToe()\n",
    "            # sets the rewards to zero\n",
    "            rewards = 0\n",
    "\n",
    "            # define a variable to indicate if there is a winner\n",
    "            winner = -1\n",
    "            # change players order\n",
    "            players = (players[1], players[0])\n",
    "            # define the current player index\n",
    "            player_idx = 1\n",
    "\n",
    "            # if we can still play\n",
    "            while winner < 0 and game.is_still_playable():\n",
    "                # change player\n",
    "                player_idx = (player_idx + 1) % 2\n",
    "                player = players[player_idx]\n",
    "\n",
    "                # define a variable to check if the chosen move is ok or not\n",
    "                ok = False\n",
    "                # if it is our turn\n",
    "                if self == player:\n",
    "                    # while the chosen move is not ok\n",
    "                    while not ok:\n",
    "                        # get the current state representation\n",
    "                        state_repr_index = self._map_state_to_index(game)\n",
    "                        # get a move\n",
    "                        move = self._make_move(game)\n",
    "                        # reshape the move to form an index\n",
    "                        action = move[0] * 3 + move[1]\n",
    "                        # perform the move and get the reward\n",
    "                        reward, ok = self._move_reward(game, move, player_idx)\n",
    "                        # get the next state representation\n",
    "                        new_state_repr_index = self._map_state_to_index(game)\n",
    "\n",
    "                        # update the action-value function\n",
    "                        self._update_q_table(state_repr_index, new_state_repr_index, action, reward)\n",
    "\n",
    "                        # update the rewards\n",
    "                        rewards += reward\n",
    "                # if it is the opponent turn\n",
    "                else:\n",
    "                    # while the chosen move is not ok\n",
    "                    while not ok:\n",
    "                        # get a move\n",
    "                        move = player.make_move(game)\n",
    "                        # perform the move\n",
    "                        ok = game.move(move, player_idx)\n",
    "\n",
    "                # check if there is a winner\n",
    "                winner = game.check_winner()\n",
    "\n",
    "            # update the exploration rate\n",
    "            self._exploration_rate = np.clip(\n",
    "                np.exp(-self._exploration_decay_rate * episode), self._min_exploration_rate, 1\n",
    "            )\n",
    "            # get the game reward\n",
    "            reward = self._game_reward(player, winner)\n",
    "            # update the action-value function\n",
    "            self._update_q_table(state_repr_index, new_state_repr_index, action, reward)\n",
    "            # update the rewards\n",
    "            rewards += reward\n",
    "            # update the rewards history\n",
    "            all_rewards.append(rewards)\n",
    "            pbar.set_description(f'rewards value: {rewards}, current exploration rate: {self._exploration_rate:2f}')\n",
    "\n",
    "        print(f'** Last 1_000 episodes - Mean rewards value: {sum(all_rewards[-1_000:]) / 1_000:.2f} **')\n",
    "        print(f'** Last rewards value: {all_rewards[-1]:} **')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rewards value: 13, current exploration rate: 0.223131: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500000/500000 [09:39<00:00, 862.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Last 1_000 episodes - Mean rewards value: 11.12 **\n",
      "** Last rewards value: 13 **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create the Q-learning player\n",
    "q_learning_rl_agent = QLearningRLPlayer(\n",
    "    n_episodes=500_000,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    min_exploration_rate=0.01,\n",
    "    exploration_decay_rate=3e-6,\n",
    "    opponent=RandomPlayer(),\n",
    ")\n",
    "# train the Q-learning player\n",
    "q_learning_rl_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of explored states: 5478\n"
     ]
    }
   ],
   "source": [
    "# print the number of explored states\n",
    "print(f'Number of explored states: {len(q_learning_rl_agent._q_table.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the Q-learning player\n",
    "with open('./q_learning_rl_agent.pkl', 'wb') as f:\n",
    "    pickle.dump(q_learning_rl_agent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the serialized Q-learning player\n",
    "with open('./q_learning_rl_agent.pkl', 'rb') as f:\n",
    "    q_learning_rl_agent = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning ðŸ†š Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['â¬œ' 'â¬œ' 'â¬œ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']]\n",
      "[['âŒ' 'â¬œ' 'â¬œ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']]\n",
      "[['âŒ' 'â¬œ' 'â¬œ']\n",
      " ['â¬œ' 'â­•' 'â¬œ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']]\n",
      "[['âŒ' 'â¬œ' 'â¬œ']\n",
      " ['â¬œ' 'â­•' 'â¬œ']\n",
      " ['â¬œ' 'âŒ' 'â¬œ']]\n",
      "[['âŒ' 'â¬œ' 'â¬œ']\n",
      " ['â­•' 'â­•' 'â¬œ']\n",
      " ['â¬œ' 'âŒ' 'â¬œ']]\n",
      "[['âŒ' 'â¬œ' 'âŒ']\n",
      " ['â­•' 'â­•' 'â¬œ']\n",
      " ['â¬œ' 'âŒ' 'â¬œ']]\n",
      "[['âŒ' 'â¬œ' 'âŒ']\n",
      " ['â­•' 'â­•' 'â­•']\n",
      " ['â¬œ' 'âŒ' 'â¬œ']]\n",
      "Winner: Player 1\n"
     ]
    }
   ],
   "source": [
    "# let the Q-learning player play against a random player\n",
    "# Q-learning player is second to move\n",
    "game = TicTacToe()\n",
    "player1 = RandomPlayer()\n",
    "player2 = q_learning_rl_agent\n",
    "winner = play(game, player1, player2, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning plays as first player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 996 wins, 0 losses and 4 draw\n",
      "Wins + Draws percentage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# let the Q-learning player play 1_000 games against a random player\n",
    "# Q-learning player is first to move\n",
    "show_statistics(0, q_learning_rl_agent, RandomPlayer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning plays as second player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 819 wins, 0 losses and 181 draw\n",
      "Wins + Draws percentage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# let the Q-learning player play 1_000 games against a random player\n",
    "# Q-learning player is second to move\n",
    "show_statistics(1, RandomPlayer(), q_learning_rl_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: Monte Carlo-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo-learning** is a reinforcement learning technique which is based on updating the action-value function $Q(s_t, a_t)$ according to the following formula:\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow \\cfrac{1}{N} \\sum_{i = 1}^{N} G_i (s_t, a_t)\n",
    "$$\n",
    "where $N$ is the number of episodes and $G_i (s_t, a_t)$ is the return of rewards experienced at the $i$-th episode for the first occurrence of $(s_t, a_t)$ in that episode. \\\n",
    "The action-value function is updated iteratively over the $N$ episodes.\n",
    "\n",
    "The cells below define a Monte Carlo-learning player and train it against a random player. \\\n",
    "The obtained results are also shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloRLPlayer(Player):\n",
    "    \"\"\"\n",
    "    Class representing player who learns to play thanks to the Monte Carlo-learning technique.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_episodes: int,\n",
    "        gamma: float,\n",
    "        min_exploration_rate: float,\n",
    "        exploration_decay_rate: float,\n",
    "        opponent: 'Player',\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        The Monte Carlo-learning player constructor.\n",
    "\n",
    "        Args:\n",
    "            n_episodes: the number of episodes for the training phase;\n",
    "            gamma: the discount rate of the Bellman equation;\n",
    "            min_exploration_rate: the minimum rate for exploration during the training phase;\n",
    "            exploration_decay_rate: the exploration decay rate used during the training;\n",
    "            opponent: the opponent to play against.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._q_table = {}  # define the Action-value function\n",
    "        self._q_counters = {}  # define counters for the return of rewards\n",
    "        self._n_episodes = n_episodes  # define the number of episodes for the training phase\n",
    "        self._gamma = gamma  # define the discount rate of the Bellman equation\n",
    "        self._exploration_rate = 1  # define the exploration rate for the training phase\n",
    "        self._min_exploration_rate = (\n",
    "            min_exploration_rate  # define the minimum rate for exploration during the training phase\n",
    "        )\n",
    "        self._exploration_decay_rate = (\n",
    "            exploration_decay_rate  # define the exploration decay rate used during the training\n",
    "        )\n",
    "        self._opponent = opponent  # define the opponent to play against\n",
    "\n",
    "    def _move_reward(self, game: 'TicTacToe', move: tuple[int, int], player_id: int) -> tuple[Literal[-1, 1], bool]:\n",
    "        \"\"\"\n",
    "        Try a move and return the corresponding reward.\n",
    "\n",
    "        Args:\n",
    "            game: a Tic-Tac-Toe game instance;\n",
    "            move: the move to try;\n",
    "            player_id: my player's id.\n",
    "\n",
    "        Returns:\n",
    "            The reward and the acceptability of the move are returned.\n",
    "        \"\"\"\n",
    "        # play a move\n",
    "        acceptable = game.move(move, player_id)\n",
    "        # give a negative reward to the agent\n",
    "        reward = -1\n",
    "        # if the move is acceptable\n",
    "        if acceptable:\n",
    "            # give a positive reward to the agent\n",
    "            reward = 1\n",
    "        return reward, acceptable\n",
    "\n",
    "    def _game_reward(self, player: 'TicTacToe', winner: int) -> Literal[-10, 0, 10]:\n",
    "        \"\"\"\n",
    "        Calculate the reward based on how the game ended.\n",
    "\n",
    "        Args:\n",
    "            player: the winning player;\n",
    "            winner: the winner's player id.\n",
    "\n",
    "        Returns:\n",
    "            The game reward is returned.\n",
    "        \"\"\"\n",
    "        # if there was no winner\n",
    "        if winner == -1:\n",
    "            # return no reward\n",
    "            return 0\n",
    "        # if the agent is the winner\n",
    "        elif self == player:\n",
    "            # give a big positive reward\n",
    "            return 10\n",
    "        # give a big negative reward, otherwise\n",
    "        return -10\n",
    "\n",
    "    def _map_state_to_index(self, game: 'TicTacToe') -> str:\n",
    "        \"\"\"\n",
    "        Given a game state, this function translates it into an index to access the Q_table.\n",
    "\n",
    "        Args:\n",
    "            game: a Tic-Tac-Toe game instance;\n",
    "            player_id: my player's id.\n",
    "        \"\"\"\n",
    "        # take the current game state\n",
    "        state = game.board\n",
    "        # change not taken tiles values to 2\n",
    "        state[state == -1] = 2\n",
    "        # map the state to a string in base 3\n",
    "        state_repr_index = ''.join(str(_) for _ in state.flatten())\n",
    "        return state_repr_index\n",
    "\n",
    "    def _update_q_table(self, state_repr_index: str, action: int, return_of_rewards: float) -> None:\n",
    "        \"\"\"\n",
    "        Update the Q_table according to the Monte Carlo-learning technique.\n",
    "\n",
    "        Args:\n",
    "            state_repr_index: the current state index;\n",
    "            action: the performed action;\n",
    "            return_of_rewards: the return of rewards for the current state.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        # if the current state is unknown\n",
    "        if state_repr_index not in self._q_counters:\n",
    "            # create its entry in the action-value mapping table\n",
    "            self._q_table[state_repr_index] = np.zeros((9,))\n",
    "            # create its entry in the counters of the return of rewards\n",
    "            self._q_counters[state_repr_index] = np.zeros((9,))\n",
    "        # update the counters of the return of rewards\n",
    "        self._q_counters[state_repr_index][action] += 1\n",
    "        # update the action-value mapping table\n",
    "        self._q_table[state_repr_index][action] = (\n",
    "            self._q_table[state_repr_index][action]\n",
    "            + (return_of_rewards - self._q_table[state_repr_index][action]) / self._q_counters[state_repr_index][action]\n",
    "        )\n",
    "\n",
    "    def _make_move(self, game: 'TicTacToe') -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Construct a move during the training phase to update the Q_table.\n",
    "\n",
    "        Args:\n",
    "            game: a Tic-Tac-Toe game instance;\n",
    "            player_id: my player's id.\n",
    "\n",
    "        Returns:\n",
    "            A move to play is returned.\n",
    "        \"\"\"\n",
    "        # get the current state representation\n",
    "        state_repr_index = self._map_state_to_index(game)\n",
    "\n",
    "        # randomly perform exploration\n",
    "        if random() < self._exploration_rate:\n",
    "            # by returning a random move\n",
    "            move = randint(0, 8)\n",
    "        # perform eploitation, otherwise\n",
    "        else:\n",
    "            # if the current state is unknown\n",
    "            if state_repr_index not in self._q_table:\n",
    "                # create its entry in the action-value mapping table\n",
    "                self._q_table[state_repr_index] = np.zeros((9,))\n",
    "                # create its entry in the counters of the return of rewards\n",
    "                self._q_counters[state_repr_index] = np.zeros((9,))\n",
    "            # take the action with maximum return of rewards\n",
    "            move = np.argmax(self._q_table[state_repr_index])\n",
    "\n",
    "        # reshape the move to match the board shape\n",
    "        move = move // 3, move % 3\n",
    "\n",
    "        return move\n",
    "\n",
    "    def make_move(self, game: 'TicTacToe') -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Construct a move to be played according to the Q_table.\n",
    "\n",
    "        Args:\n",
    "            game: a Tic-Tac-Toe game instance;\n",
    "            player_id: my player's id.\n",
    "\n",
    "        Returns:\n",
    "            A move to play is returned.\n",
    "        \"\"\"\n",
    "        # get the current state representation\n",
    "        state_repr_index = self._map_state_to_index(game)\n",
    "        # if the current state is known\n",
    "        if state_repr_index in self._q_table:\n",
    "            # take the action with maximum return of rewards\n",
    "            move = np.argmax(self._q_table[state_repr_index])\n",
    "            # reshape the move to match the board shape\n",
    "            move = move // 3, move % 3\n",
    "            # if the move is acceptable\n",
    "            if game.is_acceptable(move):\n",
    "                # return it\n",
    "                return move\n",
    "        # perform a random move, otherwise\n",
    "        return (randint(0, game.board.shape[0] - 1), randint(0, game.board.shape[1] - 1))\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Train the Monte Carlo-learning player.\n",
    "\n",
    "        Args:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        # define the history of rewards\n",
    "        all_rewards = []\n",
    "        # define how many episodes to run\n",
    "        pbar = trange(self._n_episodes)\n",
    "        # define the players\n",
    "        players = (self, self._opponent)\n",
    "\n",
    "        # for each episode\n",
    "        for episode in pbar:\n",
    "            # define a new game\n",
    "            game = TicTacToe()\n",
    "            # sets the rewards to zero\n",
    "            rewards = 0\n",
    "\n",
    "            # define the trajectory\n",
    "            trajectory = []\n",
    "\n",
    "            # define a variable to indicate if there is a winner\n",
    "            winner = -1\n",
    "            # swap players order\n",
    "            players = (players[1], players[0])\n",
    "            # define the current player index\n",
    "            player_idx = 1\n",
    "\n",
    "            # if we can still play\n",
    "            while winner < 0 and game.is_still_playable():\n",
    "                # change player\n",
    "                player_idx = (player_idx + 1) % 2\n",
    "                player = players[player_idx]\n",
    "\n",
    "                # define a variable to check if the chosen move is ok or not\n",
    "                ok = False\n",
    "                # if it is our turn\n",
    "                if self == player:\n",
    "                    # while the chosen move is not ok\n",
    "                    while not ok:\n",
    "                        # get the current state representation\n",
    "                        state_repr_index = self._map_state_to_index(game)\n",
    "                        # get a move\n",
    "                        move = self._make_move(game)\n",
    "                        # reshape the move to form an index\n",
    "                        action = move[0] * 3 + move[1]\n",
    "                        # perform the move and get the reward\n",
    "                        reward, ok = self._move_reward(game, move, player_idx)\n",
    "\n",
    "                        # update the trajectory\n",
    "                        trajectory.append((state_repr_index, action, reward))\n",
    "\n",
    "                        # update the rewards\n",
    "                        rewards += reward\n",
    "                # if it is the opponent turn\n",
    "                else:\n",
    "                    # while the chosen move is not ok\n",
    "                    while not ok:\n",
    "                        # get a move\n",
    "                        move = player.make_move(game)\n",
    "                        # perform the move\n",
    "                        ok = game.move(move, player_idx)\n",
    "\n",
    "                # check if there is a winner\n",
    "                winner = game.check_winner()\n",
    "\n",
    "            # update the exploration rate\n",
    "            self._exploration_rate = np.clip(\n",
    "                np.exp(-self._exploration_decay_rate * episode), self._min_exploration_rate, 1\n",
    "            )\n",
    "            # delete last reward\n",
    "            rewards -= reward\n",
    "            # delete last tuple in trajectory\n",
    "            trajectory.pop()\n",
    "            # get the game reward\n",
    "            reward = self._game_reward(player, winner)\n",
    "            # update the trajectory\n",
    "            trajectory.append((state_repr_index, action, reward))\n",
    "            # update the rewards\n",
    "            rewards += reward\n",
    "            # update the rewards history\n",
    "            all_rewards.append(rewards)\n",
    "\n",
    "            # set the current return of rewards\n",
    "            return_of_rewards = 0\n",
    "            # for all tuples in trajectory\n",
    "            for state_repr_index, action, reward in trajectory:\n",
    "                # update the return of rewards\n",
    "                return_of_rewards = reward + self._gamma * return_of_rewards\n",
    "                # update the action-value function\n",
    "                self._update_q_table(state_repr_index, action, return_of_rewards)\n",
    "\n",
    "            pbar.set_description(f'rewards value: {rewards}, current exploration rate: {self._exploration_rate:2f}')\n",
    "\n",
    "        print(f'** Last 1_000 episodes - Mean rewards value: {sum(all_rewards[-1_000:]) / 1_000:.2f} **')\n",
    "        print(f'** Last rewards value: {all_rewards[-1]:} **')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rewards value: 12, current exploration rate: 0.082087: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [01:56<00:00, 859.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Last 1_000 episodes - Mean rewards value: 8.55 **\n",
      "** Last rewards value: 12 **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create the Monte Carlo-learning player\n",
    "monte_carlo_rl_agent = MonteCarloRLPlayer(\n",
    "    n_episodes=100_000,\n",
    "    gamma=0.99,\n",
    "    min_exploration_rate=0.01,\n",
    "    exploration_decay_rate=2.5e-5,\n",
    "    opponent=RandomPlayer(),\n",
    ")\n",
    "# train the Monte Carlo-learning player\n",
    "monte_carlo_rl_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of explored states: 4516\n"
     ]
    }
   ],
   "source": [
    "# print the number of explored states\n",
    "print(f'Number of explored states: {len(monte_carlo_rl_agent._q_table.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the Monte Carlo-learning player\n",
    "with open('./monte_carlo_rl_agent.pkl', 'wb') as f:\n",
    "    pickle.dump(monte_carlo_rl_agent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the serialized Monte Carlo-learning player\n",
    "with open('./monte_carlo_rl_agent.pkl', 'rb') as f:\n",
    "    monte_carlo_rl_agent = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo-learning ðŸ†š Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['â¬œ' 'â¬œ' 'â¬œ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']]\n",
      "[['â¬œ' 'â¬œ' 'âŒ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']]\n",
      "[['â­•' 'â¬œ' 'âŒ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']]\n",
      "[['â­•' 'âŒ' 'âŒ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']]\n",
      "[['â­•' 'âŒ' 'âŒ']\n",
      " ['â¬œ' 'â¬œ' 'â¬œ']\n",
      " ['â­•' 'â¬œ' 'â¬œ']]\n",
      "[['â­•' 'âŒ' 'âŒ']\n",
      " ['â¬œ' 'âŒ' 'â¬œ']\n",
      " ['â­•' 'â¬œ' 'â¬œ']]\n",
      "[['â­•' 'âŒ' 'âŒ']\n",
      " ['â­•' 'âŒ' 'â¬œ']\n",
      " ['â­•' 'â¬œ' 'â¬œ']]\n",
      "Winner: Player 1\n"
     ]
    }
   ],
   "source": [
    "# let the Monte Carlo-learning player play against a random player\n",
    "# Monte Carlo-learning player is second to move\n",
    "game = TicTacToe()\n",
    "player1 = RandomPlayer()\n",
    "player2 = monte_carlo_rl_agent\n",
    "winner = play(game, player1, player2, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo-learning plays as first player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 959 wins, 8 losses and 33 draw\n",
      "Wins + Draws percentage: 99.20%\n"
     ]
    }
   ],
   "source": [
    "# let the Monte Carlo-learning player play 1_000 games against a random player\n",
    "# Monte Carlo-learning player is first to move\n",
    "show_statistics(0, monte_carlo_rl_agent, RandomPlayer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo-learning plays as second player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 692 wins, 48 losses and 260 draw\n",
      "Wins + Draws percentage: 95.20%\n"
     ]
    }
   ],
   "source": [
    "# let the Monte Carlo-learning player play 1_000 games against a random player\n",
    "# Monte Carlo-learning player is second to move\n",
    "show_statistics(1, RandomPlayer(), monte_carlo_rl_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo-learning ðŸ†š Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo-learning plays as first player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 0 wins, 0 losses and 1000 draw\n",
      "Wins + Draws percentage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# let the Monte Carlo-learning player and Q-learning player play 1_000 games against each other\n",
    "# Monte Carlo-learning player is first to move\n",
    "show_statistics(0, monte_carlo_rl_agent, q_learning_rl_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning plays as first player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 1000 wins, 0 losses and 0 draw\n",
      "Wins + Draws percentage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# let the Q-learning player and Monte Carlo-learning player play 1_000 games against each other\n",
    "# Q-learning player player is first to move\n",
    "show_statistics(0, q_learning_rl_agent, monte_carlo_rl_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
