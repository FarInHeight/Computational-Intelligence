{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: Sunday, December 17 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "* Reviews: Dies Natalis Solis Invicti ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from abc import ABC, abstractmethod\n",
    "from copy import deepcopy\n",
    "from itertools import combinations\n",
    "from random import randint, random\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self) -> None:\n",
    "        # define the board game\n",
    "        self._board = np.ones((3, 3), dtype=np.uint8) * -1\n",
    "        # define a board to ease the check winner computation\n",
    "        self._eqv_board = np.array([[1, 6, 5], [8, 4, 0], [3, 2, 7]], dtype=np.uint8)\n",
    "        # define a mapping for pretty printing\n",
    "        self._id_to_block = {-1: '⬜️', 0: '❌', 1: '⭕️'}\n",
    "\n",
    "    @property\n",
    "    def board(self) -> np.ndarray:\n",
    "        # return a copy of the board so that the board cannot be modified from outside\n",
    "        return deepcopy(self._board)\n",
    "\n",
    "    def print(self):\n",
    "        # define a board for pretty printing\n",
    "        fancy_board = np.chararray(self._board.shape, itemsize=1, unicode=True)\n",
    "        for i in range(fancy_board.shape[0]):\n",
    "            for j in range(fancy_board.shape[1]):\n",
    "                # fill the fancy board\n",
    "                fancy_board[(i, j)] = self._id_to_block[self._board[(i, j)]]\n",
    "        print(fancy_board)\n",
    "\n",
    "    def check_winner(self) -> int:\n",
    "        # take the tiles belonging to the first player\n",
    "        player1_tiles = self._board == 0\n",
    "        # take the tiles belonging to the second player\n",
    "        player2_tiles = self._board == 1\n",
    "        # check if the first player has won\n",
    "        if any(sum(h) == 12 for h in combinations(self._eqv_board[player1_tiles], 3)):\n",
    "            return 0\n",
    "        # check if the second player has won\n",
    "        if any(sum(h) == 12 for h in combinations(self._eqv_board[player2_tiles], 3)):\n",
    "            return 1\n",
    "        # no player has won\n",
    "        return -1\n",
    "\n",
    "    def is_still_playable(self):\n",
    "        # check if still there are not taken tiles\n",
    "        return any((self._board == -1).flatten())\n",
    "\n",
    "    def move(self, move: tuple[int, int], player_id: int) -> bool:\n",
    "        # if the player id is not valid\n",
    "        if player_id >= 2 or player_id <= -1:\n",
    "            return False\n",
    "        # check if the move is acceptable\n",
    "        acceptable = self.is_acceptable(move)\n",
    "        # if it is\n",
    "        if acceptable:\n",
    "            # update the board\n",
    "            self._board[move] = player_id\n",
    "        return acceptable\n",
    "\n",
    "    def is_acceptable(self, move: tuple[int, int]):\n",
    "        acceptable: bool = move[0] >= 0 and move[0] <= 3 and move[1] >= 0 and move[1] <= 3 and self._board[move] < 0\n",
    "        return acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(game: 'TicTacToe', player1: 'Player', player2: 'Player') -> int:\n",
    "    # define the players\n",
    "    players = [player1, player2]\n",
    "    # set the moving player index\n",
    "    current_player_idx = 1\n",
    "    # define a variable to indicate if there is a winner\n",
    "    winner = -1\n",
    "    # if we can still play\n",
    "    while winner < 0 and game.is_still_playable():\n",
    "        # update the current moving player index\n",
    "        current_player_idx += 1\n",
    "        current_player_idx %= len(players)\n",
    "        # define a variable to check if the chosen move is ok or not\n",
    "        ok = False\n",
    "        # while the chosen move is not ok\n",
    "        while not ok:\n",
    "            # let the current player make a move\n",
    "            move = players[current_player_idx].make_move(game, current_player_idx)\n",
    "            # check if now it is ok\n",
    "            ok = game.move(move, current_player_idx)\n",
    "        # check if there is a winner\n",
    "        winner = game.check_winner()\n",
    "    # return the winner\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_statistics(player_id: int, player1: 'Player', player2: 'Player', n_matches: int = 1_000):\n",
    "    counter_wins = 0\n",
    "    counter_losses = 0\n",
    "    counter_draws = 0\n",
    "    for _ in range(1_000):\n",
    "        game = TicTacToe()\n",
    "        winner = play(game, player1, player2)\n",
    "        counter_wins = counter_wins + 1 if winner == player_id else counter_wins\n",
    "        counter_losses = counter_losses + 1 if winner == (player_id + 1) % 2 else counter_losses\n",
    "        counter_draws = counter_draws + 1 if winner == -1 else counter_draws\n",
    "    print(f'Over {n_matches} matches: {counter_wins} wins, {counter_losses} losses and {counter_draws} draw')\n",
    "    print(f'Wins + Draws percentage: {(counter_wins + counter_draws) / n_matches:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_move(self, game: 'TicTacToe', player_id: int) -> tuple[int, int]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def make_move(self, game: TicTacToe, player_id: int) -> tuple[int, int]:\n",
    "        # return a random move\n",
    "        return (randint(0, game.board.shape[0] - 1), randint(0, game.board.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningRLPlayer(Player):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_episodes: int,\n",
    "        alpha: float,\n",
    "        gamma: float,\n",
    "        min_exploration_rate: float,\n",
    "        exploration_decay_rate: float,\n",
    "        opponent: 'Player',\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._q_table = {}  # define the Action-value function\n",
    "        self._n_episodes = n_episodes  # define the number of episodes for the training phase\n",
    "        self._alpha = alpha  # define how much information to incorporate from the new experience\n",
    "        self._gamma = gamma  # define the discount rate of the Bellman equation\n",
    "        self._exploration_rate = 1  # define the exploration rate for the training phase\n",
    "        self._min_exploration_rate = (\n",
    "            min_exploration_rate  # define the minimum rate for exploration during the training phase\n",
    "        )\n",
    "        self._exploration_decay_rate = (\n",
    "            exploration_decay_rate  # define the exploration decay rate used during the training\n",
    "        )\n",
    "        self._opponent = opponent  # define the opponent to play against\n",
    "\n",
    "    def _move_reward(self, game: 'TicTacToe', move: tuple[int, int], player_id: int) -> int:\n",
    "        # play a move\n",
    "        acceptable = game.move(move, player_id)\n",
    "        # give a negative reward to the agent\n",
    "        reward = -1\n",
    "        # if the move is acceptable\n",
    "        if acceptable:\n",
    "            # give a positive reward to the agent\n",
    "            reward = 1\n",
    "        return reward, acceptable\n",
    "\n",
    "    def _game_reward(self, player: 'TicTacToe', winner: int) -> int:\n",
    "        # if there was no winner\n",
    "        if winner == -1:\n",
    "            # return no reward\n",
    "            return 0\n",
    "        # if the agent is the winner\n",
    "        elif self == player:\n",
    "            # give a big positive reward\n",
    "            return 10\n",
    "        # give a big negative reward, otherwise\n",
    "        return -10\n",
    "\n",
    "    def _map_state_to_index(self, game: 'TicTacToe', player_id: int) -> int:\n",
    "        # take the current game state\n",
    "        state = game.board\n",
    "        # change not taken tiles values to 2\n",
    "        state[state == -1] = 2\n",
    "        # map the state to a string in base 3\n",
    "        state_repr_index = ''.join(str(_) for _ in state.flatten()) + str(player_id)\n",
    "        return state_repr_index\n",
    "\n",
    "    def _update_q_table(self, state_repr_index: str, new_state_repr_index: str, action: int, reward: int) -> None:\n",
    "        # if the current state is unknown\n",
    "        if state_repr_index not in self._q_table:\n",
    "            # create its entry in the action-value mapping table\n",
    "            self._q_table[state_repr_index] = np.zeros((9,))\n",
    "        # if the next state is unknown\n",
    "        if new_state_repr_index not in self._q_table:\n",
    "            # create its entry in the action-value mapping table\n",
    "            self._q_table[new_state_repr_index] = np.zeros((9,))\n",
    "        prev_value = self._q_table[state_repr_index][action]\n",
    "        # update the action-value mapping entry for the current state using Q-learning\n",
    "        self._q_table[state_repr_index][action] = (1 - self._alpha) * prev_value + self._alpha * (\n",
    "            reward + self._gamma * np.max(self._q_table[new_state_repr_index])\n",
    "        )\n",
    "\n",
    "    def _make_move(self, game: 'TicTacToe', player_id: int) -> tuple[int, int]:\n",
    "        # get the current state representation\n",
    "        state_repr_index = self._map_state_to_index(game, player_id)\n",
    "\n",
    "        # randomly perform exploration\n",
    "        if random() < self._exploration_rate:\n",
    "            # by returning a random move\n",
    "            move = randint(0, 8)\n",
    "        # perform eploitation, otherwise\n",
    "        else:\n",
    "            # if the current state is unknown\n",
    "            if state_repr_index not in self._q_table:\n",
    "                # create its entry in the action-value mapping table\n",
    "                self._q_table[state_repr_index] = np.zeros((9,))\n",
    "            # take the action with maximum return of rewards\n",
    "            move = np.argmax(self._q_table[state_repr_index])\n",
    "\n",
    "        # reshape the move to match the board shape\n",
    "        move = move // 3, move % 3\n",
    "\n",
    "        return move\n",
    "\n",
    "    def make_move(self, game: 'TicTacToe', player_id: int) -> tuple[int, int]:\n",
    "        # get the current state representation\n",
    "        state_repr_index = self._map_state_to_index(game, player_id)\n",
    "        # if the current state is known\n",
    "        if state_repr_index in self._q_table:\n",
    "            # take the action with maximum return of rewards\n",
    "            move = np.argmax(self._q_table[state_repr_index])\n",
    "            # reshape the move to match the board shape\n",
    "            move = move // 3, move % 3\n",
    "            # if the move is acceptable\n",
    "            if game.is_acceptable(move):\n",
    "                # return it\n",
    "                return move\n",
    "        # perform a random move, otherwise\n",
    "        return (randint(0, game.board.shape[0] - 1), randint(0, game.board.shape[1] - 1))\n",
    "\n",
    "    def train(self) -> None:\n",
    "        # define the history of rewards\n",
    "        all_rewards = []\n",
    "        # define how many episodes to run\n",
    "        pbar = trange(self._n_episodes)\n",
    "        # define the players\n",
    "        players = (self, self._opponent)\n",
    "        # for each episode\n",
    "        for episode in pbar:\n",
    "            # define a new game\n",
    "            game = TicTacToe()\n",
    "            # sets the rewards to zero\n",
    "            rewards = 0\n",
    "\n",
    "            # define a variable to indicate if there is a winner\n",
    "            winner = -1\n",
    "            # change players order\n",
    "            players = (players[1], players[0])\n",
    "            # define the current player index\n",
    "            player_idx = 1\n",
    "\n",
    "            # if we can still play\n",
    "            while winner < 0 and game.is_still_playable():\n",
    "                # change player\n",
    "                player_idx = (player_idx + 1) % 2\n",
    "                player = players[player_idx]\n",
    "\n",
    "                # define a variable to check if the chosen move is ok or not\n",
    "                ok = False\n",
    "                # if it is our turn\n",
    "                if self == player:\n",
    "                    # while the chosen move is not ok\n",
    "                    while not ok:\n",
    "                        # get the current state representation\n",
    "                        state_repr_index = self._map_state_to_index(game, player_idx)\n",
    "                        # get a move\n",
    "                        move = self._make_move(game, player_idx)\n",
    "                        # reshape the move to form an index\n",
    "                        action = move[0] * 3 + move[1]\n",
    "                        # perform the move and get the reward\n",
    "                        reward, ok = self._move_reward(game, move, player_idx)\n",
    "                        # get the next state representation\n",
    "                        new_state_repr_index = self._map_state_to_index(game, player_idx)\n",
    "\n",
    "                        # update the action-value function\n",
    "                        self._update_q_table(state_repr_index, new_state_repr_index, action, reward)\n",
    "\n",
    "                        # update the rewards\n",
    "                        rewards += reward\n",
    "                # if it is the opponent turn\n",
    "                else:\n",
    "                    # while the chosen move is not ok\n",
    "                    while not ok:\n",
    "                        # get a move\n",
    "                        move = player.make_move(game, player_idx)\n",
    "                        # perform the move\n",
    "                        ok = game.move(move, player_idx)\n",
    "\n",
    "                # check if there is a winner\n",
    "                winner = game.check_winner()\n",
    "\n",
    "            # update the exploration rate\n",
    "            self._exploration_rate = np.clip(\n",
    "                np.exp(-self._exploration_decay_rate * episode), self._min_exploration_rate, 1\n",
    "            )\n",
    "            # get the game reward\n",
    "            reward = self._game_reward(player, winner)\n",
    "            # update the action-value function\n",
    "            self._update_q_table(state_repr_index, new_state_repr_index, action, reward)\n",
    "            # update the rewards\n",
    "            rewards += reward\n",
    "            # update the rewards history\n",
    "            all_rewards.append(rewards)\n",
    "            pbar.set_description(f'rewards value: {rewards}, current exploration rate: {self._exploration_rate:2f}')\n",
    "\n",
    "        print(f'** Last 1_000 episodes - Mean rewards value: {sum(all_rewards[-1_000:]) / 1_000:.2f} **')\n",
    "        print(f'** Last rewards value: {all_rewards[-1]:} **')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rewards value: 14, current exploration rate: 0.082087: 100%|██████████| 100000/100000 [01:35<00:00, 1043.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Last 1_000 episodes - Mean rewards value: 10.25 **\n",
      "** Last rewards value: 14 **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q_learning_rl_agent = QLearningRLPlayer(\n",
    "    n_episodes=100_000,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    min_exploration_rate=0.01,\n",
    "    exploration_decay_rate=2.5e-5,\n",
    "    opponent=RandomPlayer(),\n",
    ")\n",
    "q_learning_rl_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9977"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q_learning_rl_agent._q_table.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./q_learning_rl_agent.pkl', 'wb') as f:\n",
    "    pickle.dump(q_learning_rl_agent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./q_learning_rl_agent.pkl', 'rb') as f:\n",
    "    q_learning_rl_agent = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['⬜' '⬜' '⬜']\n",
      " ['⬜' '⬜' '⬜']\n",
      " ['⬜' '⬜' '⬜']]\n",
      "[['❌' '❌' '❌']\n",
      " ['⬜' '❌' '⭕']\n",
      " ['⬜' '⭕' '⭕']]\n",
      "Winner: Player 0\n"
     ]
    }
   ],
   "source": [
    "game = TicTacToe()\n",
    "game.print()\n",
    "player1 = RandomPlayer()\n",
    "player2 = q_learning_rl_agent\n",
    "winner = play(game, player1, player2)\n",
    "game.print()\n",
    "if winner == -1:\n",
    "    print(f\"Draw!\")\n",
    "else:\n",
    "    print(f\"Winner: Player {winner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 899 wins, 22 losses and 79 draw\n",
      "Wins + Draws percentage: 97.80%\n"
     ]
    }
   ],
   "source": [
    "show_statistics(0, q_learning_rl_agent, RandomPlayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 655 wins, 68 losses and 277 draw\n",
      "Wins + Draws percentage: 93.20%\n"
     ]
    }
   ],
   "source": [
    "show_statistics(1, RandomPlayer(), q_learning_rl_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: Monte Carlo learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloRLPlayer(Player):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_episodes: int,\n",
    "        gamma: float,\n",
    "        min_exploration_rate: float,\n",
    "        exploration_decay_rate: float,\n",
    "        opponent: 'Player',\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._q_table = {}  # define the Action-value function\n",
    "        self._q_counters = {}\n",
    "        self._n_episodes = n_episodes  # define the number of episodes for the training phase\n",
    "        self._gamma = gamma  # define the discount rate of the Bellman equation\n",
    "        self._exploration_rate = 1  # define the exploration rate for the training phase\n",
    "        self._min_exploration_rate = (\n",
    "            min_exploration_rate  # define the minimum rate for exploration during the training phase\n",
    "        )\n",
    "        self._exploration_decay_rate = (\n",
    "            exploration_decay_rate  # define the exploration decay rate used during the training\n",
    "        )\n",
    "        self._opponent = opponent  # define the opponent to play against\n",
    "\n",
    "    def _move_reward(self, game: 'TicTacToe', move: tuple[int, int], player_id: int) -> int:\n",
    "        # play a move\n",
    "        acceptable = game.move(move, player_id)\n",
    "        # give a negative reward to the agent\n",
    "        reward = -1\n",
    "        # if the move is acceptable\n",
    "        if acceptable:\n",
    "            # give a positive reward to the agent\n",
    "            reward = 1\n",
    "        return reward, acceptable\n",
    "\n",
    "    def _game_reward(self, player: 'TicTacToe', winner: int) -> int:\n",
    "        # if there was no winner\n",
    "        if winner == -1:\n",
    "            # return no reward\n",
    "            return 0\n",
    "        # if the agent is the winner\n",
    "        elif self == player:\n",
    "            # give a big positive reward\n",
    "            return 10\n",
    "        # give a big negative reward, otherwise\n",
    "        return -10\n",
    "\n",
    "    def _map_state_to_index(self, game: 'TicTacToe', player_id: int) -> int:\n",
    "        # take the current game state\n",
    "        state = game.board\n",
    "        # change not taken tiles values to 2\n",
    "        state[state == -1] = 2\n",
    "        # map the state to a string in base 3\n",
    "        state_repr_index = ''.join(str(_) for _ in state.flatten()) + str(player_id)\n",
    "        return state_repr_index\n",
    "\n",
    "    def _update_q_table(self, state_repr_index: str, action: int, return_of_rewards: float) -> None:\n",
    "        # if the current state is unknown\n",
    "        if state_repr_index not in self._q_counters:\n",
    "            # create its entry in the action-value mapping table\n",
    "            self._q_table[state_repr_index] = np.zeros((9,))\n",
    "            self._q_counters[state_repr_index] = np.zeros((9,))\n",
    "        self._q_counters[state_repr_index][action] += 1\n",
    "        self._q_table[state_repr_index][action] = (\n",
    "            self._q_table[state_repr_index][action]\n",
    "            + (return_of_rewards - self._q_table[state_repr_index][action]) / self._q_counters[state_repr_index][action]\n",
    "        )\n",
    "\n",
    "    def _make_move(self, game: 'TicTacToe', player_id: int) -> tuple[int, int]:\n",
    "        # get the current state representation\n",
    "        state_repr_index = self._map_state_to_index(game, player_id)\n",
    "\n",
    "        # randomly perform exploration\n",
    "        if random() < self._exploration_rate:\n",
    "            # by returning a random move\n",
    "            move = randint(0, 8)\n",
    "        # perform eploitation, otherwise\n",
    "        else:\n",
    "            # if the current state is unknown\n",
    "            if state_repr_index not in self._q_table:\n",
    "                # create its entry in the action-value mapping table\n",
    "                self._q_table[state_repr_index] = np.zeros((9,))\n",
    "                self._q_counters[state_repr_index] = np.zeros((9,))\n",
    "            # take the action with maximum return of rewards\n",
    "            move = np.argmax(self._q_table[state_repr_index])\n",
    "\n",
    "        # reshape the move to match the board shape\n",
    "        move = move // 3, move % 3\n",
    "\n",
    "        return move\n",
    "\n",
    "    def make_move(self, game: 'TicTacToe', player_id: int) -> tuple[int, int]:\n",
    "        # get the current state representation\n",
    "        state_repr_index = self._map_state_to_index(game, player_id)\n",
    "        # if the current state is known\n",
    "        if state_repr_index in self._q_table:\n",
    "            # take the action with maximum return of rewards\n",
    "            move = np.argmax(self._q_table[state_repr_index])\n",
    "            # reshape the move to match the board shape\n",
    "            move = move // 3, move % 3\n",
    "            # if the move is acceptable\n",
    "            if game.is_acceptable(move):\n",
    "                # return it\n",
    "                return move\n",
    "        # perform a random move, otherwise\n",
    "        return (randint(0, game.board.shape[0] - 1), randint(0, game.board.shape[1] - 1))\n",
    "\n",
    "    def train(self) -> None:\n",
    "        # define the history of rewards\n",
    "        all_rewards = []\n",
    "        # define how many episodes to run\n",
    "        pbar = trange(self._n_episodes)\n",
    "        # define the players\n",
    "        players = (self, self._opponent)\n",
    "\n",
    "        # for each episode\n",
    "        for episode in pbar:\n",
    "            # define a new game\n",
    "            game = TicTacToe()\n",
    "            # sets the rewards to zero\n",
    "            rewards = 0\n",
    "\n",
    "            # define the trajectory\n",
    "            trajectory = []\n",
    "\n",
    "            # define a variable to indicate if there is a winner\n",
    "            winner = -1\n",
    "            # change players order\n",
    "            players = (players[1], players[0])\n",
    "            # define the current player index\n",
    "            player_idx = 1\n",
    "\n",
    "            # if we can still play\n",
    "            while winner < 0 and game.is_still_playable():\n",
    "                # change player\n",
    "                player_idx = (player_idx + 1) % 2\n",
    "                player = players[player_idx]\n",
    "\n",
    "                # define a variable to check if the chosen move is ok or not\n",
    "                ok = False\n",
    "                # if it is our turn\n",
    "                if self == player:\n",
    "                    # while the chosen move is not ok\n",
    "                    while not ok:\n",
    "                        # get the current state representation\n",
    "                        state_repr_index = self._map_state_to_index(game, player_idx)\n",
    "                        # get a move\n",
    "                        move = self._make_move(game, player_idx)\n",
    "                        # reshape the move to form an index\n",
    "                        action = move[0] * 3 + move[1]\n",
    "                        # perform the move and get the reward\n",
    "                        reward, ok = self._move_reward(game, move, player_idx)\n",
    "\n",
    "                        # update the trajectory\n",
    "                        trajectory.append((state_repr_index, action, reward))\n",
    "\n",
    "                        # update the rewards\n",
    "                        rewards += reward\n",
    "                # if it is the opponent turn\n",
    "                else:\n",
    "                    # while the chosen move is not ok\n",
    "                    while not ok:\n",
    "                        # get a move\n",
    "                        move = player.make_move(game, player_idx)\n",
    "                        # perform the move\n",
    "                        ok = game.move(move, player_idx)\n",
    "\n",
    "                # check if there is a winner\n",
    "                winner = game.check_winner()\n",
    "\n",
    "            # update the exploration rate\n",
    "            self._exploration_rate = np.clip(\n",
    "                np.exp(-self._exploration_decay_rate * episode), self._min_exploration_rate, 1\n",
    "            )\n",
    "            # delete last reward\n",
    "            rewards -= reward\n",
    "            # delete last tuple in trajectory\n",
    "            trajectory.pop()\n",
    "            # get the game reward\n",
    "            reward = self._game_reward(player, winner)\n",
    "            # update the trajectory\n",
    "            trajectory.append((state_repr_index, action, reward))\n",
    "            # update the rewards\n",
    "            rewards += reward\n",
    "            # update the rewards history\n",
    "            all_rewards.append(rewards)\n",
    "\n",
    "            # set the current return of rewards\n",
    "            return_of_rewards = 0\n",
    "            # for all tuples in trajectory\n",
    "            for state_repr_index, action, reward in trajectory:\n",
    "                # update the return of rewards\n",
    "                return_of_rewards = reward + self._gamma * return_of_rewards\n",
    "                # update the action-value function\n",
    "                self._update_q_table(state_repr_index, action, return_of_rewards)\n",
    "\n",
    "            pbar.set_description(f'rewards value: {rewards}, current exploration rate: {self._exploration_rate:2f}')\n",
    "\n",
    "        print(f'** Last 1_000 episodes - Mean rewards value: {sum(all_rewards[-1_000:]) / 1_000:.2f} **')\n",
    "        print(f'** Last rewards value: {all_rewards[-1]:} **')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rewards value: 12, current exploration rate: 0.082087: 100%|██████████| 100000/100000 [01:30<00:00, 1109.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Last 1_000 episodes - Mean rewards value: 7.98 **\n",
      "** Last rewards value: 12 **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "monte_carlo_rl_agent = MonteCarloRLPlayer(\n",
    "    n_episodes=100_000,\n",
    "    gamma=0.99,\n",
    "    min_exploration_rate=0.01,\n",
    "    exploration_decay_rate=2.5e-5,\n",
    "    opponent=RandomPlayer(),\n",
    ")\n",
    "monte_carlo_rl_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4519"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(monte_carlo_rl_agent._q_table.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./monte_carlo_rl_agent.pkl', 'wb') as f:\n",
    "    pickle.dump(monte_carlo_rl_agent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./monte_carlo_rl_agent.pkl', 'rb') as f:\n",
    "    monte_carlo_rl_agent = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['⬜' '⬜' '⬜']\n",
      " ['⬜' '⬜' '⬜']\n",
      " ['⬜' '⬜' '⬜']]\n",
      "[['⬜' '⭕' '⬜']\n",
      " ['❌' '⭕' '❌']\n",
      " ['⬜' '⭕' '❌']]\n",
      "Winner: Player 1\n"
     ]
    }
   ],
   "source": [
    "game = TicTacToe()\n",
    "game.print()\n",
    "player1 = RandomPlayer()\n",
    "player2 = monte_carlo_rl_agent\n",
    "winner = play(game, player1, player2)\n",
    "game.print()\n",
    "if winner == -1:\n",
    "    print(f\"Draw!\")\n",
    "else:\n",
    "    print(f\"Winner: Player {winner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 946 wins, 13 losses and 41 draw\n",
      "Wins + Draws percentage: 98.70%\n"
     ]
    }
   ],
   "source": [
    "show_statistics(0, monte_carlo_rl_agent, RandomPlayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 685 wins, 67 losses and 248 draw\n",
      "Wins + Draws percentage: 93.30%\n"
     ]
    }
   ],
   "source": [
    "show_statistics(1, RandomPlayer(), monte_carlo_rl_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 0 wins, 0 losses and 1000 draw\n",
      "Wins + Draws percentage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "show_statistics(0, monte_carlo_rl_agent, q_learning_rl_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 1000 wins, 0 losses and 0 draw\n",
      "Wins + Draws percentage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "show_statistics(0, q_learning_rl_agent, monte_carlo_rl_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 0 wins, 1000 losses and 0 draw\n",
      "Wins + Draws percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "show_statistics(1, q_learning_rl_agent, monte_carlo_rl_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
