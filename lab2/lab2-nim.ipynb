{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Lab 2: ES\n",
    "\n",
    "## Task\n",
    "\n",
    "Write agents able to play [*Nim*](https://en.wikipedia.org/wiki/Nim), with an arbitrary number of rows and an upper bound $k$ on the number of objects that can be removed in a turn (a.k.a., *subtraction game*).\n",
    "\n",
    "The goal of the game is to **avoid** taking the last object.\n",
    "\n",
    "* Task 2.1: An agent using fixed rules based on *nim-sum* (i.e., an *expert system*)\n",
    "* Task 2.2: An agent using evolved rules using ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pprint import pprint, pformat\n",
    "from collections import namedtuple\n",
    "from random import random, choice, randint\n",
    "from copy import deepcopy\n",
    "from typing import Callable, Literal\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.notebook import trange\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *Nim* and *Nimply* classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# named tuple to indicate a possible Nim ply\n",
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim:\n",
    "    \"\"\"\n",
    "    Class implementing the Nim game.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        \"\"\"\n",
    "        Game constructor.\n",
    "\n",
    "        Args:\n",
    "            num_rows: number of rows (piles);\n",
    "            k: maximum number of objects you can nim from a row.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self._k\n",
    "\n",
    "    def nimming(self, ply: Nimply) -> None:\n",
    "        \"\"\"\n",
    "        Update the game by performing a ply.\n",
    "\n",
    "        Args:\n",
    "            ply: ply to perform.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k, f\"{num_objects=}, {self._k=}\"\n",
    "        self._rows[row] -= num_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(nim: Nim, strategy1: Callable[[Nim], Nimply], strategy2: Callable[[Nim], Nimply]) -> Literal[0, 1]:\n",
    "    \"\"\"\n",
    "    Play a Nim game using the given strategies.\n",
    "\n",
    "    Args:\n",
    "        nim: Nim game instance;\n",
    "        strategy1: Player 0 strategy;\n",
    "        strategy2: Player 1 strategy.\n",
    "\n",
    "    Returns:\n",
    "        player: the winning player.\n",
    "    \"\"\"\n",
    "    logging.getLogger().setLevel(logging.WARN)\n",
    "\n",
    "    strategy = (strategy1, strategy2)\n",
    "\n",
    "    logging.info(f\"init : {nim}\")\n",
    "    player = 0\n",
    "    while nim:\n",
    "        ply = strategy[player](nim)\n",
    "        logging.info(f\"ply: player {player} plays {ply}, {nim_sum(nim)}\")\n",
    "        nim.nimming(ply)\n",
    "        logging.info(f\"status: {nim}\")\n",
    "        player = 1 - player\n",
    "    logging.info(f\"status: Player {player} won!\")\n",
    "\n",
    "    return player\n",
    "\n",
    "\n",
    "def play_games(\n",
    "    nim: Nim,\n",
    "    player: int,\n",
    "    player_strategy: Callable[[Nim], Nimply],\n",
    "    opponent_strategy: Callable[[Nim], Nimply],\n",
    "    n_matches: int,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Play a given number of matches on a Nim game instance.\n",
    "\n",
    "    Args:\n",
    "        nim: Nim game instance;\n",
    "        player: choose if your strategy is played by the first or second player;\n",
    "        player_strategy: your player strategy;\n",
    "        opponent_strategy: your opponent strategy;\n",
    "        n_matches: number of matchers to play.\n",
    "\n",
    "    Returns:\n",
    "        List history of the winning players.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        play_game(deepcopy(nim), player_strategy, opponent_strategy)\n",
    "        if player == 0\n",
    "        else play_game(deepcopy(nim), opponent_strategy, player_strategy)\n",
    "        for _ in range(n_matches)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streak(player_strategy, opponent_strategy, n_matches):\n",
    "    \"\"\"\n",
    "    Play a given number of random matches between two strategies.\n",
    "\n",
    "    Args:\n",
    "        player_strategy: your player strategy;\n",
    "        opponent_strategy: your opponent strategy;\n",
    "        n_matches: number of matchers to play.\n",
    "\n",
    "    Returns:\n",
    "        ercentage of wins.\n",
    "    \"\"\"\n",
    "    wins = 0\n",
    "    for _ in range(n_matches):\n",
    "        random_size = randint(4, 10)\n",
    "        random_k = choice([None, None, *[randint(2, random_size * 2 + 1) for _ in range(2)]])\n",
    "        nim = Nim(random_size, random_k)\n",
    "        player = choice([0, 1])\n",
    "        wins += 1 if play_games(nim, player, player_strategy, opponent_strategy, 1)[0] == player else 0\n",
    "    return wins / n_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based strategies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_random(state: Nim) -> Nimply:\n",
    "    \"\"\"\n",
    "    Perform a completely random move.\n",
    "\n",
    "    Args:\n",
    "        state: Nim game instance.\n",
    "\n",
    "    Returns:\n",
    "        A ply is returned.\n",
    "    \"\"\"\n",
    "    row = choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    num_objects = randint(1, state.rows[row] if state.k is None else min(state.rows[row], state.k))\n",
    "    return Nimply(row, num_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabriele(state: Nim) -> Nimply:\n",
    "    \"\"\"\n",
    "    Pick always the maximum possible number of the lowest row.\n",
    "\n",
    "    Args:\n",
    "        state: Nim game instance.\n",
    "\n",
    "    Returns:\n",
    "        A ply is returned.\n",
    "    \"\"\"\n",
    "    possible_moves = [\n",
    "        (r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1 if state.k is None else min(c + 1, state.k))\n",
    "    ]\n",
    "    return Nimply(*max(possible_moves, key=lambda m: (-m[0], m[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_sum(state: Nim) -> int:\n",
    "    \"\"\"\n",
    "    Compute nim-sum value on a Nim game instance.\n",
    "\n",
    "    Args:\n",
    "        state: Nim game instance.\n",
    "\n",
    "    Returns:\n",
    "        The nim-sum value of the current game is returned.\n",
    "    \"\"\"\n",
    "    tmp = np.array([tuple(int(x) for x in f\"{c:032b}\") for c in state.rows])\n",
    "    xor = tmp.sum(axis=0) % 2\n",
    "    return int(\"\".join(str(_) for _ in xor), base=2)\n",
    "\n",
    "\n",
    "def generate_all_plies(state: Nim) -> list[Nimply]:\n",
    "    \"\"\"\n",
    "    Generate all possible plies on the current game.\n",
    "\n",
    "    Args:\n",
    "        state: Nim game instance.\n",
    "\n",
    "    Returns:\n",
    "        A list of plies is returned.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        Nimply(r, o)\n",
    "        for r, c in enumerate(state.rows)\n",
    "        for o in range(1, c + 1 if state.k is None else min(c + 1, state.k))\n",
    "    ]\n",
    "\n",
    "\n",
    "def analize(state: Nim) -> dict:\n",
    "    \"\"\"\n",
    "    Given a Nim game instance, this function computes all the possible plies\n",
    "    and the corresponding nim-sum values.\n",
    "\n",
    "    Args:\n",
    "        state: Nim game instance.\n",
    "\n",
    "    Returns:\n",
    "        Plies and corresponding nim-sum values are returned as a dict.\n",
    "    \"\"\"\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = dict()\n",
    "    for ply in generate_all_plies(state):\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming(ply)\n",
    "        cooked[\"possible_moves\"][ply] = nim_sum(tmp)\n",
    "    return cooked\n",
    "\n",
    "\n",
    "def optimal(state: Nim) -> Nimply:\n",
    "    \"\"\"\n",
    "    If possible, this function returns a move which leads to a nim-sum value not equal to zero,\n",
    "    otherwise a random move among all the possible moves.\n",
    "\n",
    "    Args:\n",
    "        state: Nim game instance.\n",
    "\n",
    "    Returns:\n",
    "        A ply is returned.\n",
    "    \"\"\"\n",
    "    analysis = analize(state)\n",
    "    logging.debug(f\"analysis:\\n{pformat(analysis)}\")\n",
    "    spicy_moves = [ply for ply, ns in analysis[\"possible_moves\"].items() if ns != 0]\n",
    "    if not spicy_moves:\n",
    "        spicy_moves = list(analysis[\"possible_moves\"].keys())\n",
    "    logging.debug(pformat(f\"{analysis['possible_moves']}\"))\n",
    "    ply = choice(spicy_moves)\n",
    "    return ply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without considering the value of $k$, we can win in a Nim game if we always play a move in order to have a nim-sum value of $0$.\n",
    "\n",
    "At some point, we end up in a position that has only one row of size 2 or more. \\\n",
    "In such a position the nim-sum value is not equal to 0. To win we must reduce this to size 0 or 1, leaving an odd number of rows with size 1. From that point on, all moves are forced.\n",
    "\n",
    "_*Reference: https://en.wikipedia.org/wiki/Nim#Proof_of_the_winning_formula*_.\n",
    "\n",
    "The following function implements this winning strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_system(state: Nim) -> Nimply:\n",
    "    \"\"\"\n",
    "    This function implement an expert system which beats the strategies defined above.\n",
    "    Details on how to win are available at https://en.wikipedia.org/wiki/Nim#Proof_of_the_winning_formula.\n",
    "\n",
    "    Args:\n",
    "        state: Nim game instance.\n",
    "\n",
    "    Returns:\n",
    "        A ply is returned.\n",
    "    \"\"\"\n",
    "    analysis = analize(state)\n",
    "    logging.debug(f\"analysis:\\n{pformat(analysis)}\")\n",
    "    not_zero_rows = len(state.rows) - state.rows.count(0)\n",
    "    one_count_rows = state.rows.count(1)\n",
    "    if one_count_rows == not_zero_rows - 1:\n",
    "        is_odd = (one_count_rows % 2) == 1\n",
    "        row, objects = [(row, objects) for row, objects in enumerate(state.rows) if objects > 1][0]\n",
    "        if is_odd:\n",
    "            return Nimply(row, objects if state.k is None else min(objects, state.k))\n",
    "        return Nimply(row, objects - 1 if state.k is None else min(objects - 1, state.k))\n",
    "    spicy_moves = [ply for ply, ns in analysis[\"possible_moves\"].items() if ns == 0]\n",
    "    if not spicy_moves:\n",
    "        spicy_moves = list(analysis[\"possible_moves\"].keys())\n",
    "    logging.debug(pformat(f\"{analysis['possible_moves']}\"))\n",
    "    ply = choice(spicy_moves)\n",
    "    return ply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Strategy\n",
    "\n",
    "### Task 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea is to implement an adaptive strategy that learns how to play depending on the current game phase. \n",
    "\n",
    "Suppose that we have $3$ different phases:\n",
    "- _early game_;\n",
    "- _mid game_;\n",
    "- _end game_.\n",
    "\n",
    "We decide in which phase we are in based on the formula $n_\\textrm{phase} = \\dfrac{\\textrm{\\# remaining plies}}{\\textrm{\\# possible plies from the beginning}} \\in [0, 1]$.\n",
    " \n",
    "We define $t_1, t_2$, two thresholds learnt by the _ES_ strategy, that divide the interval $[0, 1]$ in three parts: $i_1 = [0, t_1)$, $i_2 = [t_1, t_2]$ and $i_3 = (t_2, 1]$. \\\n",
    "If $n_\\textrm{phase} \\in i_1$ we are in _early game_, $n_\\textrm{phase} \\in i_2$ we are in _mid game_, otherwise we are in _end game_.\n",
    "\n",
    "Each phase has its own set of weights. One weight is associated to one of the strategies defined above. The probability of picking a strategy for a ply is directly proportional to its weight value.\n",
    "\n",
    "> Note: Our first attempt was to repeatedly apply a softmax function to maintain a list of probabilities instead of a list of weights. Using this technique, each probability converges to $\\dfrac{1}{n_{\\textrm{strategies}}}$ and then it is not applicable.\n",
    "\n",
    "The _ES_ algorithm learns which strategy is best to play in each phase.\n",
    "\n",
    "An individual fitness is the percentage of wins against an `expert_system` in a random sequence of matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase_ratio(state: Nim) -> float:\n",
    "    \"\"\"\n",
    "    Get the n_phase value.\n",
    "\n",
    "    Args:\n",
    "        state: Nim game instance.\n",
    "\n",
    "    Returns:\n",
    "        The n_phase value is returned.\n",
    "    \"\"\"\n",
    "    all_plys_new_game = len(generate_all_plies(Nim(len(state.rows), state.k)))\n",
    "    all_plys_current_game = len(generate_all_plies(state))\n",
    "    return all_plys_current_game / all_plys_new_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(init=False)\n",
    "class Individual:\n",
    "    \"\"\"\n",
    "    Class that represents an individual.\n",
    "    \"\"\"\n",
    "\n",
    "    strategies: list[Callable[[Nim], Nimply]]\n",
    "    strategy_weights: list[list[float]]\n",
    "    phase_thresholds: list[float]\n",
    "\n",
    "    def __init__(\n",
    "        self, strategies: int = None, strategy_weights: list[list[float]] = None, phase_thresholds: list[float] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Individual constructor.\n",
    "\n",
    "        Args:\n",
    "            strategies: strategies to use;\n",
    "            strategy_weights: matrix with a weight for each strategy and game phase;\n",
    "            phase_thresholds: thresholds to split the game in 3 phases.\n",
    "        \"\"\"\n",
    "        if strategies is None:\n",
    "            strategies = [pure_random, gabriele, optimal, expert_system]\n",
    "        if strategy_weights is None:\n",
    "            strategy_weights = np.random.randint(low=1, high=10, size=(3, len(strategies)))\n",
    "        if phase_thresholds is None:\n",
    "            phase_thresholds = sorted([random(), random()])\n",
    "        else:\n",
    "            phase_thresholds = sorted(phase_thresholds)\n",
    "\n",
    "        self.strategies = strategies\n",
    "        self.strategy_weights = strategy_weights\n",
    "        self.phase_thresholds = phase_thresholds\n",
    "\n",
    "    def _softmax(self, values) -> list[list[float]]:\n",
    "        tmp = np.exp(values)\n",
    "        return (tmp / np.sum(tmp)).tolist()\n",
    "\n",
    "    def mutate(ind: \"Individual\") -> \"Individual\":\n",
    "        global mutation_rate\n",
    "        strategy_weights = np.clip(np.random.normal(loc=ind.strategy_weights, scale=mutation_rate[0]), 1, 10).tolist()\n",
    "        phase_thresholds = np.clip(np.random.normal(loc=ind.phase_thresholds, scale=mutation_rate[1]), 0, 1).tolist()\n",
    "        return Individual(\n",
    "            strategies=ind.strategies, strategy_weights=strategy_weights, phase_thresholds=phase_thresholds\n",
    "        )\n",
    "\n",
    "    def __call__(self, state: Nim) -> Nimply:\n",
    "        phase_ratio = get_phase_ratio(state)\n",
    "        phase_index = (\n",
    "            0 if phase_ratio < self.phase_thresholds[0] else (2 if phase_ratio > self.phase_thresholds[1] else 1)\n",
    "        )\n",
    "        weights = self.strategy_weights[phase_index]\n",
    "        strategy = np.random.choice(self.strategies, p=self._softmax(weights))\n",
    "        return strategy(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 + $\\lambda$)-ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a _(1 + $\\lambda$)-ES strategy_ we start with a single parent and in each iteration we generate $\\lambda$ new individuals (_offspring_) using only random mutations performed on the parent. \\\n",
    "We then evaluate the quality of the new individuals and decide whether it is better to keep the parent or to update it, i.e. a new individual becomes the parent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 30\n",
    "N_MATCHES = 10\n",
    "N_ITERS = 1_000\n",
    "FACTOR = 1.5\n",
    "OPPONENT = expert_system\n",
    "mutation_rate: tuple[float] = (2.5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- First Individual\n",
      "Individual(strategies=[<function pure_random at 0x1096c9760>,\n",
      "                       <function gabriele at 0x1096c98a0>,\n",
      "                       <function optimal at 0x1096c9b20>,\n",
      "                       <function expert_system at 0x1096c9580>],\n",
      "           strategy_weights=array([[2, 2, 3, 9],\n",
      "       [3, 7, 7, 4],\n",
      "       [1, 5, 2, 9]]),\n",
      "           phase_thresholds=[0.3178864333248911, 0.37234192826551626])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efada1b3eb3149cdb3dde01c3a85c58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Best Individual\n",
      "Individual(strategies=[<function pure_random at 0x1096c9760>,\n",
      "                       <function gabriele at 0x1096c98a0>,\n",
      "                       <function optimal at 0x1096c9b20>,\n",
      "                       <function expert_system at 0x1096c9580>],\n",
      "           strategy_weights=[[1.0546074594975139,\n",
      "                              1.2371015165152381,\n",
      "                              1.2987271315346984,\n",
      "                              8.837894831030649],\n",
      "                             [5.359184210109394,\n",
      "                              2.8403651499224076,\n",
      "                              3.6836658822228787,\n",
      "                              9.94485543681981],\n",
      "                             [4.821053743895413,\n",
      "                              2.5925847466986003,\n",
      "                              3.204851545801586,\n",
      "                              9.076811249985374]],\n",
      "           phase_thresholds=[0.3513941449954518, 0.39292677311548024])\n",
      "Best Individual Accuracy: 53.00%\n"
     ]
    }
   ],
   "source": [
    "parent = Individual()\n",
    "parent_fitness = streak(parent, OPPONENT, N_MATCHES)\n",
    "print('-- First Individual')\n",
    "pprint(parent)\n",
    "\n",
    "pbar = trange(0, N_ITERS // LAMBDA)\n",
    "for _ in pbar:\n",
    "    pbar.set_description(f'Parent Accuracy: {parent_fitness:.2%}')\n",
    "    offspring = [parent.mutate() for _ in range(LAMBDA)]\n",
    "    offspring_fitness = [streak(ind, OPPONENT, N_MATCHES) for ind in offspring]\n",
    "\n",
    "    if np.sum(np.array(offspring_fitness) > parent_fitness) / LAMBDA > 1 / 5:\n",
    "        mutation_rate = (mutation_rate[0] * FACTOR, mutation_rate[1] * FACTOR)\n",
    "    else:\n",
    "        mutation_rate = (mutation_rate[0] / FACTOR, mutation_rate[1] / FACTOR)\n",
    "\n",
    "    solution_index = np.argmax(offspring_fitness)\n",
    "    if parent_fitness < offspring_fitness[solution_index]:\n",
    "        parent = offspring[solution_index]\n",
    "        parent_fitness = offspring_fitness[solution_index]\n",
    "\n",
    "    if parent_fitness >= 0.999:\n",
    "        break\n",
    "\n",
    "best_plus = parent\n",
    "print('-- Best Individual')\n",
    "pprint(best_plus)\n",
    "print(f'Best Individual Accuracy: {streak(best_plus, OPPONENT, 100):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the best individual for the plus strategy\n",
    "with open('best_plus.pkl', 'wb') as f:\n",
    "    pickle.dump(best_plus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best individual for the plus strategy\n",
    "with open('best_plus.pkl', 'rb') as f:\n",
    "    best_plus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ($\\mu$ + $\\lambda$)-ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a _($\\mu$ + $\\lambda$)-ES strategy_ we start with $\\mu$ parents and in each iteration we generate $\\lambda$ new individuals (_offspring_) using only random mutations performed on the parents. Each parent will generate $\\dfrac{\\lambda}{\\mu}$ new individuals. \\\n",
    "We then evaluate the quality of the new individuals and update the parents by keeping only the best $\\mu$ individuals among the parents and the offspring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "MU = 3\n",
    "LAMBDA = 30\n",
    "N_MATCHES = 10\n",
    "N_ITERS = 1_000\n",
    "FACTOR = 1.5\n",
    "OPPONENT = expert_system\n",
    "mutation_rate: tuple[float] = (2.5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- First Individuals\n",
      "array([Individual(strategies=[<function pure_random at 0x1096c9760>, <function gabriele at 0x1096c98a0>, <function optimal at 0x1096c9b20>, <function expert_system at 0x1096c9580>], strategy_weights=array([[6, 9, 3, 2],\n",
      "              [6, 4, 3, 1],\n",
      "              [9, 9, 8, 3]]), phase_thresholds=[0.6458599491837625, 0.7849314920012852])                                                                                                                                   ,\n",
      "       Individual(strategies=[<function pure_random at 0x1096c9760>, <function gabriele at 0x1096c98a0>, <function optimal at 0x1096c9b20>, <function expert_system at 0x1096c9580>], strategy_weights=array([[5, 4, 9, 2],\n",
      "              [1, 6, 6, 7],\n",
      "              [9, 9, 7, 9]]), phase_thresholds=[0.3633419256594267, 0.6703733589327934])                                                                                                                                   ,\n",
      "       Individual(strategies=[<function pure_random at 0x1096c9760>, <function gabriele at 0x1096c98a0>, <function optimal at 0x1096c9b20>, <function expert_system at 0x1096c9580>], strategy_weights=array([[8, 3, 1, 2],\n",
      "              [1, 4, 4, 8],\n",
      "              [1, 7, 9, 9]]), phase_thresholds=[0.2217370260551379, 0.5849375287582789])                                                                                                                                   ],\n",
      "      dtype=object)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084b95b268624bc6a598e83789e31221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Best Individuals\n",
      "Individual(strategies=[<function pure_random at 0x1096c9760>,\n",
      "                       <function gabriele at 0x1096c98a0>,\n",
      "                       <function optimal at 0x1096c9b20>,\n",
      "                       <function expert_system at 0x1096c9580>],\n",
      "           strategy_weights=[[5.637417236034652,\n",
      "                              3.6404611215484928,\n",
      "                              3.5138873688328736,\n",
      "                              5.495421847944803],\n",
      "                             [1.1625274727520074,\n",
      "                              3.12444533004775,\n",
      "                              5.413734875616345,\n",
      "                              10.0],\n",
      "                             [2.7826479059572513,\n",
      "                              6.609153819328993,\n",
      "                              5.2727669520168154,\n",
      "                              9.42103106095656]],\n",
      "           phase_thresholds=[1.111228389619841e-06, 0.662344690194218])\n",
      "Best Individual Accuracy: 35.00%\n"
     ]
    }
   ],
   "source": [
    "map_individual_to_fitness = np.vectorize(lambda parent: streak(parent, OPPONENT, N_MATCHES))\n",
    "\n",
    "parents = np.array([Individual() for _ in range(MU)])\n",
    "parents_fitness = map_individual_to_fitness(parents)\n",
    "max_parents_fitness = np.max(parents_fitness)\n",
    "print('-- First Individuals')\n",
    "pprint(parents)\n",
    "\n",
    "pbar = trange(0, N_ITERS // LAMBDA)\n",
    "for _ in pbar:\n",
    "    pbar.set_description(f'Best Parent Accuracy: {max_parents_fitness:.2%}')\n",
    "    offspring = np.array([parent.mutate() for parent in parents for _ in range(LAMBDA // MU)])\n",
    "    offspring_fitness = map_individual_to_fitness(offspring)\n",
    "\n",
    "    if np.sum(offspring_fitness > max_parents_fitness) / LAMBDA > 1 / 5:\n",
    "        mutation_rate = (mutation_rate[0] * FACTOR, mutation_rate[1] * FACTOR)\n",
    "    else:\n",
    "        mutation_rate = (mutation_rate[0] / FACTOR, mutation_rate[1] / FACTOR)\n",
    "\n",
    "    population = np.hstack((offspring, parents))\n",
    "    population_fitness = np.hstack((offspring_fitness, parents_fitness))\n",
    "    population_fitness_indexes = np.argsort(population_fitness)\n",
    "    parents = population[population_fitness_indexes][::-1][:MU]\n",
    "    parents_fitness = population_fitness[population_fitness_indexes][::-1][:MU]\n",
    "    max_parents_fitness = np.max(parents_fitness)\n",
    "\n",
    "    if max_parents_fitness >= 0.999:\n",
    "        break\n",
    "\n",
    "best_plus_mu_lambda = choice(parents[parents_fitness == max_parents_fitness])\n",
    "print('-- Best Individuals')\n",
    "pprint(best_plus_mu_lambda)\n",
    "print(f'Best Individual Accuracy: {streak(best_plus_mu_lambda, OPPONENT, 100):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the best individual for the plus (mu + lambda) strategy\n",
    "with open('best_plus_mu_lambda.pkl', 'wb') as f:\n",
    "    pickle.dump(best_plus_mu_lambda, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best individual for the plus (mu + lambda) strategy\n",
    "with open('best_plus_mu_lambda.pkl', 'rb') as f:\n",
    "    best_plus_mu_lambda = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1, $\\lambda$)-ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a _(1, $\\lambda$)-ES strategy_ we start with a single parent and in each iteration we generate $\\lambda$ new individuals (_offspring_) using only random mutations performed on the parent. \\\n",
    "We then evaluate the quality of the new individuals and the best individual in the offspring becomes the new parent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 30\n",
    "N_MATCHES = 10\n",
    "N_ITERS = 1_000\n",
    "FACTOR = 1.1\n",
    "OPPONENT = expert_system\n",
    "mutation_rate: tuple[float] = (1.5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- First Individual\n",
      "Individual(strategies=[<function pure_random at 0x1096c9760>,\n",
      "                       <function gabriele at 0x1096c98a0>,\n",
      "                       <function optimal at 0x1096c9b20>,\n",
      "                       <function expert_system at 0x1096c9580>],\n",
      "           strategy_weights=array([[9, 7, 9, 9],\n",
      "       [1, 5, 8, 2],\n",
      "       [7, 9, 2, 4]]),\n",
      "           phase_thresholds=[0.7719534138669364, 0.8310978528484756])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef2fcf1fe6248139bfdf8c9b947d111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Best Individual\n",
      "Individual(strategies=[<function pure_random at 0x1096c9760>,\n",
      "                       <function gabriele at 0x1096c98a0>,\n",
      "                       <function optimal at 0x1096c9b20>,\n",
      "                       <function expert_system at 0x1096c9580>],\n",
      "           strategy_weights=[[4.664006878649911,\n",
      "                              3.8736983447138247,\n",
      "                              5.220779969398328,\n",
      "                              9.79578265740777],\n",
      "                             [2.8252679397185076,\n",
      "                              3.294736753635436,\n",
      "                              3.856527981637476,\n",
      "                              5.791523798828324],\n",
      "                             [2.306070214180621,\n",
      "                              5.320849194254264,\n",
      "                              2.3766687139005955,\n",
      "                              8.726438276922705]],\n",
      "           phase_thresholds=[0.8467489248038672, 0.927095654088388])\n",
      "Best Individual Accuracy: 41.00%\n"
     ]
    }
   ],
   "source": [
    "parent = Individual()\n",
    "parent_fitness = streak(parent, OPPONENT, N_MATCHES)\n",
    "best_comma = parent\n",
    "best_comma_fitness = parent_fitness\n",
    "print('-- First Individual')\n",
    "pprint(parent)\n",
    "\n",
    "pbar = trange(0, N_ITERS // LAMBDA)\n",
    "for _ in pbar:\n",
    "    pbar.set_description(f'Parent Accuracy: {parent_fitness:.2%}')\n",
    "    offspring = [parent.mutate() for _ in range(LAMBDA)]\n",
    "    offspring_fitness = [streak(ind, OPPONENT, N_MATCHES) for ind in offspring]\n",
    "\n",
    "    if np.sum(np.array(offspring_fitness) > parent_fitness) / LAMBDA > 1 / 5:\n",
    "        mutation_rate = (mutation_rate[0] * FACTOR, mutation_rate[1] * FACTOR)\n",
    "    else:\n",
    "        mutation_rate = (mutation_rate[0] / FACTOR, mutation_rate[1] / FACTOR)\n",
    "\n",
    "    solution_index = np.argmax(offspring_fitness)\n",
    "    parent = offspring[solution_index]\n",
    "    parent_fitness = offspring_fitness[solution_index]\n",
    "\n",
    "    if best_comma_fitness < parent_fitness:\n",
    "        best_comma = parent\n",
    "        best_comma_fitness = parent_fitness\n",
    "\n",
    "    if parent_fitness >= 0.999:\n",
    "        break\n",
    "\n",
    "print('-- Best Individual')\n",
    "pprint(best_comma)\n",
    "print(f'Best Individual Accuracy: {streak(best_comma, OPPONENT, 100):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the best individual for the comma strategy\n",
    "with open('best_comma.pkl', 'wb') as f:\n",
    "    pickle.dump(best_comma, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best individual for the comma strategy\n",
    "with open('best_comma.pkl', 'rb') as f:\n",
    "    best_comma = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ($\\mu$, $\\lambda$)-ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a _($\\mu$, $\\lambda$)-ES strategy_ we start with $\\mu$ parents and in each iteration we generate $\\lambda$ new individuals (_offspring_) using only random mutations performed on the parents. Each parent will generate $\\dfrac{\\lambda}{\\mu}$ new individuals. \\\n",
    "We then evaluate the quality of the new individuals and update the parents by keeping only the best $\\mu$ individuals in the offspring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "MU = 3\n",
    "LAMBDA = 30\n",
    "N_MATCHES = 10\n",
    "N_ITERS = 1_000\n",
    "FACTOR = 1.5\n",
    "OPPONENT = expert_system\n",
    "mutation_rate: tuple[float] = (2.5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- First Individuals\n",
      "array([Individual(strategies=[<function pure_random at 0x1096c9760>, <function gabriele at 0x1096c98a0>, <function optimal at 0x1096c9b20>, <function expert_system at 0x1096c9580>], strategy_weights=array([[8, 7, 9, 5],\n",
      "              [3, 6, 3, 8],\n",
      "              [2, 5, 7, 3]]), phase_thresholds=[0.538656898774971, 0.7547411767064064])                                                                                                                                    ,\n",
      "       Individual(strategies=[<function pure_random at 0x1096c9760>, <function gabriele at 0x1096c98a0>, <function optimal at 0x1096c9b20>, <function expert_system at 0x1096c9580>], strategy_weights=array([[4, 1, 7, 3],\n",
      "              [8, 7, 8, 3],\n",
      "              [1, 8, 9, 3]]), phase_thresholds=[0.06652628499690783, 0.7627991617592504])                                                                                                                                  ,\n",
      "       Individual(strategies=[<function pure_random at 0x1096c9760>, <function gabriele at 0x1096c98a0>, <function optimal at 0x1096c9b20>, <function expert_system at 0x1096c9580>], strategy_weights=array([[3, 9, 3, 9],\n",
      "              [2, 8, 4, 3],\n",
      "              [9, 5, 1, 7]]), phase_thresholds=[0.7600634186047154, 0.9400005591356998])                                                                                                                                   ],\n",
      "      dtype=object)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f813f04c2c854eababf4ba4b03aa7c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Best Individuals\n",
      "Individual(strategies=[<function pure_random at 0x1096c9760>,\n",
      "                       <function gabriele at 0x1096c98a0>,\n",
      "                       <function optimal at 0x1096c9b20>,\n",
      "                       <function expert_system at 0x1096c9580>],\n",
      "           strategy_weights=[[4.5539151576395955,\n",
      "                              2.745291289717418,\n",
      "                              2.5529870687978877,\n",
      "                              9.525133044134293],\n",
      "                             [1.844380179897061,\n",
      "                              7.26207565382174,\n",
      "                              8.224640343139457,\n",
      "                              4.913202339337638],\n",
      "                             [6.911161153550008,\n",
      "                              1.7293993326010413,\n",
      "                              4.285519853688302,\n",
      "                              9.705183146898264]],\n",
      "           phase_thresholds=[0.7905139250241182, 0.8113032777940925])\n",
      "Best Individual Accuracy: 38.00%\n"
     ]
    }
   ],
   "source": [
    "map_individual_to_fitness = np.vectorize(lambda parent: streak(parent, OPPONENT, N_MATCHES))\n",
    "\n",
    "parents = np.array([Individual() for _ in range(MU)])\n",
    "parents_fitness = map_individual_to_fitness(parents)\n",
    "max_parents_fitness = np.max(parents_fitness)\n",
    "best_comma_mu_lambda = choice(parents[parents_fitness == max_parents_fitness])\n",
    "best_comma_mu_lambda_fitness = max_parents_fitness\n",
    "print('-- First Individuals')\n",
    "pprint(parents)\n",
    "\n",
    "pbar = trange(0, N_ITERS // LAMBDA)\n",
    "for _ in pbar:\n",
    "    pbar.set_description(f'Best Parent Accuracy: {max_parents_fitness:.2%}')\n",
    "    offspring = np.array([parent.mutate() for parent in parents for _ in range(LAMBDA // MU)])\n",
    "    offspring_fitness = map_individual_to_fitness(offspring)\n",
    "\n",
    "    if np.sum(offspring_fitness > max_parents_fitness) / LAMBDA > 1 / 5:\n",
    "        mutation_rate = (mutation_rate[0] * FACTOR, mutation_rate[1] * FACTOR)\n",
    "    else:\n",
    "        mutation_rate = (mutation_rate[0] / FACTOR, mutation_rate[1] / FACTOR)\n",
    "\n",
    "    offspring_fitness_indexes = np.argsort(offspring_fitness)\n",
    "    parents = offspring[offspring_fitness_indexes][::-1][:MU]\n",
    "    parents_fitness = offspring_fitness[offspring_fitness_indexes][::-1][:MU]\n",
    "    max_parents_fitness = np.max(parents_fitness)\n",
    "\n",
    "    if best_comma_mu_lambda_fitness < max_parents_fitness:\n",
    "        best_comma_mu_lambda = choice(parents[parents_fitness == max_parents_fitness])\n",
    "        best_comma_mu_lambda_fitness = max_parents_fitness\n",
    "\n",
    "    if max_parents_fitness >= 0.999:\n",
    "        break\n",
    "\n",
    "print('-- Best Individuals')\n",
    "pprint(best_comma_mu_lambda)\n",
    "print(f'Best Individual Accuracy: {streak(best_comma_mu_lambda, OPPONENT, 100):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the best individual for the comma (mu, lambda) strategy\n",
    "with open('best_comma_mu_lambda.pkl', 'wb') as f:\n",
    "    pickle.dump(best_comma_mu_lambda, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best individual for the comma (mu, lambda) strategy\n",
    "with open('best_comma_mu_lambda.pkl', 'rb') as f:\n",
    "    best_comma_mu_lambda = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Since our *evolved agents* include `expert_system` among the strategies they can use, we expect its weight to be the highest in all phases of the game after a few generations. \n",
    "\n",
    "\n",
    "In particular, we can sometimes see that `expert_system` does not have the highest weight in some phases. This is because the population has learned to discard some phases by making their interval negligible. In these cases, the corresponding strategy weights are never taken into account.\n",
    "\n",
    "All in all, each agent will play the best possible strategy most of the time, with the exception of a few rounds that could lead to victory. If `expert_system` expects us to play the best possible play and we do not, the game could go in our favour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_strategy(\n",
    "    player_strategy: Callable[[Nim], Nimply],\n",
    "    opponent_strategies: list[Callable[[Nim], Nimply]],\n",
    "    n_matches: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    This function prints how many times the given player strategy\n",
    "    wins against the opponent strategies by playing a certain number\n",
    "    of random Nim games.\n",
    "\n",
    "    Args:\n",
    "        player_strategy: which strategy to play;\n",
    "        opponent_strategies: which strategies to play against;\n",
    "        n_matches: number of matches to assess the quality of the strategy.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    for opponent_strategy in opponent_strategies:\n",
    "        accuracy = streak(player_strategy, opponent_strategy, n_matches)\n",
    "        print(f\"-- Player {player_strategy.__qualname__} against {opponent_strategy.__qualname__}: {accuracy:.2%} wins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expert System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Player expert_system against pure_random: 97.30% wins\n",
      "-- Player expert_system against gabriele: 98.70% wins\n",
      "-- Player expert_system against optimal: 97.80% wins\n"
     ]
    }
   ],
   "source": [
    "assess_strategy(\n",
    "    player_strategy=expert_system,\n",
    "    opponent_strategies=[pure_random, gabriele, optimal],\n",
    "    n_matches=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 + $\\lambda$)-ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Player adaptive strategy (1 + 𝜆)-ES against pure_random: 97.40% wins\n",
      "-- Player adaptive strategy (1 + 𝜆)-ES against gabriele: 99.10% wins\n",
      "-- Player adaptive strategy (1 + 𝜆)-ES against optimal: 98.20% wins\n",
      "-- Player adaptive strategy (1 + 𝜆)-ES against expert_system: 48.10% wins\n"
     ]
    }
   ],
   "source": [
    "best_plus.__qualname__ = \"adaptive strategy (1 + 𝜆)-ES\"\n",
    "\n",
    "assess_strategy(\n",
    "    player_strategy=best_plus,\n",
    "    opponent_strategies=[pure_random, gabriele, optimal, expert_system],\n",
    "    n_matches=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaptive strategy (1 + 𝜆)-ES performance as Player 0 against expert_system on the 100-match Nim(5, 3) competition: 59.00%\n"
     ]
    }
   ],
   "source": [
    "best_plus.__qualname__ = \"adaptive strategy (1 + 𝜆)-ES\"\n",
    "player = randint(0, 1)\n",
    "accuracy = (\n",
    "    play_games(\n",
    "        nim=Nim(5, 3),\n",
    "        player=player,\n",
    "        player_strategy=best_plus,\n",
    "        opponent_strategy=expert_system,\n",
    "        n_matches=100,\n",
    "    ).count(player)\n",
    "    / 100\n",
    ")\n",
    "print(\n",
    "    f'{best_plus.__qualname__} performance as Player {player} against {expert_system.__qualname__} on the 100-match Nim(5, 3) competition: {accuracy:.2%}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ($\\mu$ + $\\lambda$)-ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Player adaptive strategy (μ + 𝜆)-ES against pure_random: 96.10% wins\n",
      "-- Player adaptive strategy (μ + 𝜆)-ES against gabriele: 98.10% wins\n",
      "-- Player adaptive strategy (μ + 𝜆)-ES against optimal: 97.50% wins\n",
      "-- Player adaptive strategy (μ + 𝜆)-ES against expert_system: 42.50% wins\n"
     ]
    }
   ],
   "source": [
    "best_plus_mu_lambda.__qualname__ = \"adaptive strategy (μ + 𝜆)-ES\"\n",
    "\n",
    "assess_strategy(\n",
    "    player_strategy=best_plus_mu_lambda,\n",
    "    opponent_strategies=[pure_random, gabriele, optimal, expert_system],\n",
    "    n_matches=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaptive strategy (μ + 𝜆)-ES performance as Player 0 against expert_system on the 100-match Nim(5, 3) competition: 60.00%\n"
     ]
    }
   ],
   "source": [
    "best_plus_mu_lambda.__qualname__ = \"adaptive strategy (μ + 𝜆)-ES\"\n",
    "player = randint(0, 1)\n",
    "accuracy = (\n",
    "    play_games(\n",
    "        nim=Nim(5, 3),\n",
    "        player=player,\n",
    "        player_strategy=best_plus_mu_lambda,\n",
    "        opponent_strategy=expert_system,\n",
    "        n_matches=100,\n",
    "    ).count(player)\n",
    "    / 100\n",
    ")\n",
    "print(\n",
    "    f'{best_plus_mu_lambda.__qualname__} performance as Player {player} against {expert_system.__qualname__} on the 100-match Nim(5, 3) competition: {accuracy:.2%}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1, $\\lambda$)-ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Player adaptive strategy (1, 𝜆)-ES against pure_random: 96.10% wins\n",
      "-- Player adaptive strategy (1, 𝜆)-ES against gabriele: 98.60% wins\n",
      "-- Player adaptive strategy (1, 𝜆)-ES against optimal: 97.80% wins\n",
      "-- Player adaptive strategy (1, 𝜆)-ES against expert_system: 41.30% wins\n"
     ]
    }
   ],
   "source": [
    "best_comma.__qualname__ = \"adaptive strategy (1, 𝜆)-ES\"\n",
    "\n",
    "assess_strategy(\n",
    "    player_strategy=best_comma,\n",
    "    opponent_strategies=[pure_random, gabriele, optimal, expert_system],\n",
    "    n_matches=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaptive strategy (1, 𝜆)-ES performance as Player 0 against expert_system on the 100-match Nim(5, 3) competition: 65.00%\n"
     ]
    }
   ],
   "source": [
    "best_comma.__qualname__ = \"adaptive strategy (1, 𝜆)-ES\"\n",
    "player = randint(0, 1)\n",
    "accuracy = (\n",
    "    play_games(\n",
    "        nim=Nim(5, 3),\n",
    "        player=player,\n",
    "        player_strategy=best_comma,\n",
    "        opponent_strategy=expert_system,\n",
    "        n_matches=100,\n",
    "    ).count(player)\n",
    "    / 100\n",
    ")\n",
    "print(\n",
    "    f'{best_comma.__qualname__} performance as Player {player} against {expert_system.__qualname__} on the 100-match Nim(5, 3) competition: {accuracy:.2%}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ($\\mu$, $\\lambda$)-ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Player adaptive strategy (μ, 𝜆)-ES against pure_random: 96.90% wins\n",
      "-- Player adaptive strategy (μ, 𝜆)-ES against gabriele: 98.00% wins\n",
      "-- Player adaptive strategy (μ, 𝜆)-ES against optimal: 98.10% wins\n",
      "-- Player adaptive strategy (μ, 𝜆)-ES against expert_system: 41.00% wins\n"
     ]
    }
   ],
   "source": [
    "best_comma_mu_lambda.__qualname__ = \"adaptive strategy (μ, 𝜆)-ES\"\n",
    "\n",
    "assess_strategy(\n",
    "    player_strategy=best_comma_mu_lambda,\n",
    "    opponent_strategies=[pure_random, gabriele, optimal, expert_system],\n",
    "    n_matches=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaptive strategy (μ, 𝜆)-ES performance as Player 0 against expert_system on the 100-match Nim(5, 3) competition: 59.00%\n"
     ]
    }
   ],
   "source": [
    "best_comma_mu_lambda.__qualname__ = \"adaptive strategy (μ, 𝜆)-ES\"\n",
    "player = randint(0, 1)\n",
    "accuracy = (\n",
    "    play_games(\n",
    "        nim=Nim(5, 3),\n",
    "        player=player,\n",
    "        player_strategy=best_comma_mu_lambda,\n",
    "        opponent_strategy=expert_system,\n",
    "        n_matches=100,\n",
    "    ).count(player)\n",
    "    / 100\n",
    ")\n",
    "print(\n",
    "    f'{best_comma_mu_lambda.__qualname__} performance as Player {player} against {expert_system.__qualname__} on the 100-match Nim(5, 3) competition: {accuracy:.2%}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result analysis\n",
    "\n",
    "As we expected, the _ES_ strategies developed in the sections above play better than any other _rule-based strategy_ except for `expert_system`.\n",
    "\n",
    "Against `expert_system`, we get a percentage of wins nearly equal to $50\\%$. \n",
    "\n",
    "My idea is that we should have achieved at least a value greater than $50\\%$ because we have more knowledge than the implemented `expert_system`, in the sense that we can theoretically modulate which strategy to use, and we can occasionally fool the opponent with an unexpected ply. \\\n",
    "This increase in results could be obtained by tuning the hyperparameters of each _ES_ strategy or by increasing the number of generations (iterations)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('ci22-dPIXJ0_o-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "10197e8e2f2aa67e2c349105091c77f4cd384fce4877865f002d9ec653f96bc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
